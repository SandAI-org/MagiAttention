# Copyright (c) 2025 SandAI. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass

import torch

import magi_attention
from magi_attention.common import AttnRanges


@dataclass(repr=False)
class AttnArg:
    q_ranges: AttnRanges
    k_ranges: AttnRanges
    attn_type_map: list[int]

    # REVIEW(xiaowu): Is shard_seqlen_q an appropriate name?
    shard_seqlen_q: int

    # total area of the attn arg, -1 means unknown
    total_area: int = -1

    # NOTE: The following variables are automatically generated by `__post_init__`
    # and serve as the meta arguments for ffa.
    # max_seqlen_q: int
    # max_seqlen_k: int
    # skip_attn: bool
    # q_ranges_tensor: torch.Tensor
    # k_ranges_tensor: torch.Tensor
    # attn_type_map_tensor: torch.Tensor

    def __post_init__(self):
        # shape check
        assert (
            len(self.q_ranges) == len(self.k_ranges) == len(self.attn_type_map)
        ), f"{len(self.q_ranges)=}, {len(self.k_ranges)=}, {len(self.attn_type_map)=}"

        # filter empty, and overwrite the original inputs
        # REVIEW: whether this is still necessary
        self._filter_out_empty_slice()

        # init fwd ffa args dict
        # NOTE: we need to init fwd args first before bwd args
        self._init_ffa_fwd_args_dict()

        # init ffa args for bwd
        self._init_ffa_bwd_args_dict()

    def _filter_out_empty_slice(self) -> None:
        filtered_q_ranges = AttnRanges()
        filtered_k_ranges = AttnRanges()
        filtered_attn_type_map: list[int] = []

        # filter out k_ranges with seqlen == 0
        for q_range, k_range, attn_type_map in zip(
            self.q_ranges, self.k_ranges, self.attn_type_map
        ):
            if not k_range.is_empty():
                filtered_q_ranges.append(q_range)
                filtered_k_ranges.append(k_range)
                filtered_attn_type_map.append(attn_type_map)

        # overwrite the original inputs
        (
            self.q_ranges,
            self.k_ranges,
            self.attn_type_map,
        ) = (
            filtered_q_ranges,
            filtered_k_ranges,
            filtered_attn_type_map,
        )

        # sanity check
        if magi_attention.is_sanity_check_enable():
            # shape check
            assert len(self.q_ranges) == len(self.k_ranges) == len(self.attn_type_map)
            # check non-empty k ranges
            for k_range in self.k_ranges:
                assert not k_range.is_empty()

    def _init_ffa_fwd_args_dict(self) -> None:
        # init `skip_attn_fwd` flag
        batch_size_fwd = len(self.q_ranges)
        self.skip_attn_fwd = batch_size_fwd == 0

        # init `disable_fwd_atomic_reduction` flag
        self.disable_fwd_atomic_reduction = self.q_ranges.is_non_overlap()

        # init tensors
        q_ranges_tensor_fwd = self.q_ranges.to_tensor(
            device=torch.cuda.current_device()
        )
        k_ranges_tensor_fwd = self.k_ranges.to_tensor(
            device=torch.cuda.current_device()
        )
        mask_type_tensor_fwd = torch.tensor(
            self.attn_type_map,
            dtype=torch.int32,
            device=torch.cuda.current_device(),
        )

        # sanity check
        if magi_attention.is_sanity_check_enable():
            # check tensor shape
            if not self.skip_attn_fwd:
                assert q_ranges_tensor_fwd.shape == torch.Size([batch_size_fwd, 2])
                assert k_ranges_tensor_fwd.shape == torch.Size([batch_size_fwd, 2])
                assert mask_type_tensor_fwd.shape == torch.Size([batch_size_fwd])

        # init max seqlen
        if self.skip_attn_fwd:  # no calc needed
            max_seqlen_q_fwd = 0
            max_seqlen_k_fwd = 0
        else:
            max_seqlen_q_fwd = self.q_ranges.max_seqlen
            max_seqlen_k_fwd = self.k_ranges.max_seqlen

        self.ffa_fwd_args_dict = dict(
            q_ranges=q_ranges_tensor_fwd,
            k_ranges=k_ranges_tensor_fwd,
            attn_type_map=mask_type_tensor_fwd,
            max_seqlen_q=max_seqlen_q_fwd,
            max_seqlen_k=max_seqlen_k_fwd,
        )

    def _init_ffa_bwd_args_dict(self) -> None:
        # just copy args from fwd
        self.skip_attn_bwd = self.skip_attn_fwd
        self.q_ranges_bwd = self.q_ranges
        self.k_ranges_bwd = self.k_ranges
        self.attn_type_map_bwd = self.attn_type_map

        self.ffa_bwd_args_dict = self.ffa_fwd_args_dict

        # init `disable_bwd_dkv_atomic_reduction` flag
        # FIXME: some bug with this flag, thus set to False temporarily
        # self.disable_bwd_dkv_atomic_reduction = self.k_ranges_bwd.is_non_overlap()
        self.disable_bwd_dkv_atomic_reduction = False

    def to_ffa_args(self, is_bwd: bool = False) -> dict:
        return self.ffa_bwd_args_dict if is_bwd else self.ffa_fwd_args_dict

    def can_skip(self, is_bwd: bool = False) -> bool:
        return self.skip_attn_bwd if is_bwd else self.skip_attn_fwd

    def __repr__(self) -> str:
        return (
            f"AttnArg(q_ranges={self.q_ranges}, k_ranges={self.k_ranges}, attn_type_map={self.attn_type_map}, "
            f"shard_seqlen_q={self.shard_seqlen_q}, total_area={self.total_area}"
        )


@dataclass
class AttnCalcMeta:
    local_attn_arg: AttnArg
    remote_attn_args_list: list[AttnArg]

    @property
    def overlap_degree(self) -> int:
        return len(self.remote_attn_args_list)

    def __post_init__(self):
        pass
