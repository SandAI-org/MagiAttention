# Copyright (c) 2025 SandAI. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass

import torch

import magi_attention
from magi_attention.common import AttnRanges


@dataclass(repr=False)
class AttnArg:
    q_ranges: AttnRanges
    k_ranges: AttnRanges
    attn_type_map: list[int]

    # REVIEW(xiaowu): Is shard_seqlen_q an appropriate name?
    shard_seqlen_q: int

    # total area of the attn arg, -1 means unknown
    total_area: int = -1

    # NOTE: The following variables are automatically generated by `__post_init__`
    # and serve as the meta arguments for ffa.
    # max_seqlen_q: int
    # max_seqlen_k: int
    # skip_attn: bool
    # q_ranges_tensor: torch.Tensor
    # k_ranges_tensor: torch.Tensor
    # attn_type_map_tensor: torch.Tensor
    # out_zero_fill_ranges: list[tuple[int, int]]

    def __post_init__(self):
        # shape check
        assert (
            len(self.q_ranges) == len(self.k_ranges) == len(self.attn_type_map)
        ), f"{len(self.q_ranges)=}, {len(self.k_ranges)=}, {len(self.attn_type_map)=}"
        # assert len(self.q_ranges) == len(self.k_ranges) == len(self.attn_type_map)

        # init out zero-fill ranges for fwd out correction
        # TODO: put this logic into kernel
        self._init_out_zero_fill_ranges()

        # filter empty, and overwrite the original inputs
        self._filter_out_empty_slice()

        # init fwd ffa args dict
        # NOTE: we need to init fwd args first before bwd args
        self._init_ffa_fwd_args_dict()

        # init ffa args for bwd
        self._init_ffa_bwd_args_dict()

    def _filter_out_empty_slice(self) -> None:
        filtered_q_ranges = AttnRanges()
        filtered_k_ranges = AttnRanges()
        filtered_attn_type_map: list[int] = []

        # filter out k_ranges with seqlen == 0
        for q_range, k_range, attn_type_map in zip(
            self.q_ranges, self.k_ranges, self.attn_type_map
        ):
            if not k_range.is_empty():
                filtered_q_ranges.append(q_range)
                filtered_k_ranges.append(k_range)
                filtered_attn_type_map.append(attn_type_map)

        # overwrite the original inputs
        (
            self.q_ranges,
            self.k_ranges,
            self.attn_type_map,
        ) = (
            filtered_q_ranges,
            filtered_k_ranges,
            filtered_attn_type_map,
        )

        # sanity check
        if magi_attention.is_sanity_check_enable():
            # shape check
            assert len(self.q_ranges) == len(self.k_ranges) == len(self.attn_type_map)
            # check non-empty k ranges
            for k_range in self.k_ranges:
                assert not k_range.is_empty()

    def _init_out_zero_fill_ranges(self) -> None:
        device = torch.cuda.current_device()
        shard_q_ranges = AttnRanges.from_ranges([[0, self.shard_seqlen_q]])
        self.out_zero_fill_ranges = AttnRanges.find_hole_ranges(
            shard_q_ranges,
            self.q_ranges,
            is_self_merged=True,
        ).to_naive_ranges()

        # calculate range sizes
        range_sizes = [end - start for start, end in self.out_zero_fill_ranges]

        # calculate the output size
        total_size = sum(range_sizes)

        # calculate row_map from row idx to range idx
        range_sizes = torch.tensor([0] + range_sizes, dtype=torch.int32, device=device)
        row_map = torch.repeat_interleave(
            torch.arange(0, len(self.out_zero_fill_ranges), device=device),
            range_sizes[1:],
            dim=0,
            output_size=total_size,
        )

        # calculate cu_range_sizes
        cu_range_sizes = torch.cumsum(range_sizes, dim=0)

        self.out_zero_range_fill_kwargs = {
            "ranges": torch.tensor(self.out_zero_fill_ranges, device=device),
            "cu_range_sizes": cu_range_sizes,
            "row_map": row_map,
            "total_size": total_size,
        }

    def _init_ffa_fwd_args_dict(self) -> None:
        # init `skip_attn_fwd` flag
        batch_size_fwd = len(self.q_ranges)
        self.skip_attn_fwd = batch_size_fwd == 0

        # init `disable_fwd_atomic_reduction` flag
        self.disable_fwd_atomic_reduction = self.q_ranges.is_non_overlap()

        # init tensors
        q_ranges_tensor_fwd = self.q_ranges.to_tensor(
            device=torch.cuda.current_device()
        )
        k_ranges_tensor_fwd = self.k_ranges.to_tensor(
            device=torch.cuda.current_device()
        )
        mask_type_tensor_fwd = torch.tensor(
            self.attn_type_map,
            dtype=torch.int32,
            device=torch.cuda.current_device(),
        )

        # sanity check
        if magi_attention.is_sanity_check_enable():
            # check tensor shape
            if not self.skip_attn_fwd:
                assert q_ranges_tensor_fwd.shape == torch.Size([batch_size_fwd, 2])
                assert k_ranges_tensor_fwd.shape == torch.Size([batch_size_fwd, 2])
                assert mask_type_tensor_fwd.shape == torch.Size([batch_size_fwd])

        # init max seqlen
        if self.skip_attn_fwd:  # no calc needed
            max_seqlen_q_fwd = 0
            max_seqlen_k_fwd = 0
        else:
            max_seqlen_q_fwd = self.q_ranges.max_seqlen
            max_seqlen_k_fwd = self.k_ranges.max_seqlen

        self.ffa_fwd_args_dict = dict(
            q_ranges=q_ranges_tensor_fwd,
            k_ranges=k_ranges_tensor_fwd,
            attn_type_map=mask_type_tensor_fwd,
            max_seqlen_q=max_seqlen_q_fwd,
            max_seqlen_k=max_seqlen_k_fwd,
        )

    def _init_ffa_bwd_args_dict(self) -> None:
        # just copy args from fwd
        self.skip_attn_bwd = self.skip_attn_fwd
        self.q_ranges_bwd = self.q_ranges
        self.k_ranges_bwd = self.k_ranges
        self.attn_type_map_bwd = self.attn_type_map

        self.ffa_bwd_args_dict = self.ffa_fwd_args_dict

    def to_ffa_args(self, is_bwd: bool = False) -> dict:
        return self.ffa_bwd_args_dict if is_bwd else self.ffa_fwd_args_dict

    def can_skip(self, is_bwd: bool = False) -> bool:
        return self.skip_attn_bwd if is_bwd else self.skip_attn_fwd

    def __repr__(self) -> str:
        return (
            f"AttnArg(q_ranges={self.q_ranges}, k_ranges={self.k_ranges}, attn_type_map={self.attn_type_map}, "
            f"shard_seqlen_q={self.shard_seqlen_q}, total_area={self.total_area}"
        )


@dataclass
class AttnCalcMeta:
    local_attn_arg: AttnArg
    remote_attn_args_list: list[AttnArg]

    @property
    def overlap_degree(self) -> int:
        return len(self.remote_attn_args_list)

    def __post_init__(self):
        pass
