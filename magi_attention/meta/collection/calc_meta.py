# Copyright (c) 2025 SandAI. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass

import torch

import magi_attention
from magi_attention.common import AttnRanges
from magi_attention.utils import format_dict_field, format_list_field


@dataclass(repr=False)
class AttnArg:
    q_ranges: AttnRanges
    k_ranges: AttnRanges
    attn_type_map: list[int]

    # total area of the attn arg, -1 means unknown
    total_area: int = -1

    # NOTE: The following variables are automatically generated by `__post_init__`
    # and serve as the meta arguments for ffa kernels in dist-attn.
    #   ffa_fwd_args_dict: dict
    #   skip_attn_fwd: bool
    #   disable_fwd_atomic_reduction: bool
    #
    #   q_ranges_bwd: AttnRanges
    #   k_ranges_bwd: AttnRanges
    #   attn_type_map_bwd: list[int]
    #   ffa_bwd_args_dict: dict
    #   skip_attn_bwd: bool
    #   disable_bwd_dkv_atomic_reduction: bool

    def __post_init__(self):
        # shape check
        assert len(self.q_ranges) == len(self.k_ranges) == len(self.attn_type_map)

        # init fwd ffa args dict
        # NOTE: we need to init fwd args first before bwd args
        self._init_ffa_fwd_args_dict()

        # init ffa args for bwd
        self._init_ffa_bwd_args_dict()

    def _init_ffa_fwd_args_dict(self) -> None:
        # init `skip_attn_fwd` flag
        batch_size_fwd = len(self.q_ranges)
        self.skip_attn_fwd = batch_size_fwd == 0

        # init `disable_fwd_atomic_reduction` flag
        self.disable_fwd_atomic_reduction = self.q_ranges.is_non_overlap()

        # init tensors
        q_ranges_tensor_fwd = self.q_ranges.to_tensor(
            device=torch.cuda.current_device()
        )
        k_ranges_tensor_fwd = self.k_ranges.to_tensor(
            device=torch.cuda.current_device()
        )
        mask_type_tensor_fwd = torch.tensor(
            self.attn_type_map,
            dtype=torch.int32,
            device=torch.cuda.current_device(),
        )

        # sanity check
        if magi_attention.is_sanity_check_enable():
            # check tensor shape
            if not self.skip_attn_fwd:
                assert q_ranges_tensor_fwd.shape == torch.Size([batch_size_fwd, 2])
                assert k_ranges_tensor_fwd.shape == torch.Size([batch_size_fwd, 2])
                assert mask_type_tensor_fwd.shape == torch.Size([batch_size_fwd])

        # init max seqlen
        if self.skip_attn_fwd:  # no calc needed
            max_seqlen_q_fwd = 0
            max_seqlen_k_fwd = 0
        else:
            max_seqlen_q_fwd = self.q_ranges.max_seqlen
            max_seqlen_k_fwd = self.k_ranges.max_seqlen

        self.ffa_fwd_args_dict = dict(
            q_ranges=q_ranges_tensor_fwd,
            k_ranges=k_ranges_tensor_fwd,
            attn_type_map=mask_type_tensor_fwd,
            max_seqlen_q=max_seqlen_q_fwd,
            max_seqlen_k=max_seqlen_k_fwd,
        )

    def _init_ffa_bwd_args_dict(self) -> None:
        # just copy args from fwd
        self.skip_attn_bwd = self.skip_attn_fwd
        self.q_ranges_bwd = self.q_ranges
        self.k_ranges_bwd = self.k_ranges
        self.attn_type_map_bwd = self.attn_type_map

        self.ffa_bwd_args_dict = self.ffa_fwd_args_dict

        # init `disable_bwd_dkv_atomic_reduction` flag
        # FIXME: some bug with this flag, thus set to False temporarily
        # self.disable_bwd_dkv_atomic_reduction = self.k_ranges_bwd.is_non_overlap()
        self.disable_bwd_dkv_atomic_reduction = False

    def to_ffa_args(self, is_bwd: bool = False) -> dict:
        return self.ffa_bwd_args_dict if is_bwd else self.ffa_fwd_args_dict

    def can_skip(self, is_bwd: bool = False) -> bool:
        return self.skip_attn_bwd if is_bwd else self.skip_attn_fwd

    def __repr__(self) -> str:
        indent = ""
        repr_str = "AttnArg(\n"

        repr_str += f"{indent}    q_ranges={repr(self.q_ranges)},\n"
        repr_str += f"{indent}    k_ranges={repr(self.k_ranges)},\n"
        repr_str += f"{indent}    attn_type_map={self.attn_type_map},\n"
        repr_str += f"{indent}    total_area={self.total_area},\n"

        repr_str += f"{indent}    # Generated by __post_init__ (fwd):\n"
        repr_str += format_dict_field(
            "ffa_fwd_args_dict", self.ffa_fwd_args_dict, indent
        )
        repr_str += f"{indent}    skip_attn_fwd={self.skip_attn_fwd},\n"
        repr_str += f"{indent}    disable_fwd_atomic_reduction={self.disable_fwd_atomic_reduction},\n"

        repr_str += f"{indent}    # Generated by __post_init__ (bwd):\n"
        repr_str += f"{indent}    q_ranges_bwd={repr(self.q_ranges_bwd)},\n"
        repr_str += f"{indent}    k_ranges_bwd={repr(self.k_ranges_bwd)},\n"
        repr_str += f"{indent}    attn_type_map_bwd={self.attn_type_map_bwd},\n"
        repr_str += format_dict_field(
            "ffa_bwd_args_dict", self.ffa_bwd_args_dict, indent
        )
        repr_str += f"{indent}    skip_attn_bwd={self.skip_attn_bwd},\n"
        repr_str += f"{indent}    disable_bwd_dkv_atomic_reduction={self.disable_bwd_dkv_atomic_reduction},\n"

        repr_str = repr_str.rstrip(",\n") + "\n)"
        return repr_str


@dataclass(repr=False)
class AttnCalcMeta:
    local_attn_arg: AttnArg
    remote_attn_args_list: list[AttnArg]

    @property
    def overlap_degree(self) -> int:
        return len(self.remote_attn_args_list)

    def __repr__(self) -> str:
        indent = ""
        repr_str = f"AttnCalcMeta(overlap_degree={self.overlap_degree},\n"

        # local_attn_arg
        local_attn_arg_repr_lines = repr(self.local_attn_arg).splitlines()
        repr_str += f"{indent}    local_attn_arg={local_attn_arg_repr_lines[0]}\n"
        for line in local_attn_arg_repr_lines[1:]:
            repr_str += f"{indent}    {line}\n"

        # remote_attn_args_list
        repr_str += format_list_field(
            "remote_attn_args_list", self.remote_attn_args_list, indent
        )

        repr_str = repr_str.rstrip(",\n") + "\n)"
        return repr_str
