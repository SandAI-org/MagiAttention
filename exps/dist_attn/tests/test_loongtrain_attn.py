# Copyright (c) 2025 SandAI. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass
from typing import Optional

import torch
import torch.distributed
import torch.distributed as dist
from flash_attn import flash_attn_varlen_func
from torch.testing._internal.common_distributed import skip_if_lt_x_gpu
from torch.testing._internal.common_utils import run_tests

from magi_attention.common.enum import AttnMaskType
from magi_attention.common.ranges import AttnRanges
from magi_attention.testing.dist_common import DistTestBase, with_comms

# isort: split
from exps.dist_attn.baselines.loongtrain import LoongTrain, ParallelMode


@dataclass
class LoongTrainAttnConfig:
    total_seqlen: int
    num_heads: int
    head_dim: int
    dtype: torch.dtype
    deterministic: bool
    qkv_format: str
    dropout_p: float


@dataclass
class LoongTrainAttnArg:
    seqlens: torch.Tensor
    is_causal: bool

    # Users should not set the following variables, as they are automatically generated by __post_init__.
    cu_seqlens_q: Optional[torch.Tensor] = None
    cu_seqlens_k: Optional[torch.Tensor] = None
    max_seqlen_q: Optional[int] = None
    max_seqlen_k: Optional[int] = None
    ranges_q: Optional[AttnRanges] = None
    ranges_k: Optional[AttnRanges] = None
    attn_mask_type: Optional[AttnMaskType] = None
    attention_mask: Optional[torch.Tensor] = None

    def __post_init__(self):
        device = self.seqlens.device
        self.cu_seqlens_q = self.seqlens.cumsum(dim=0, dtype=torch.int32)
        self.cu_seqlens_k = self.seqlens.cumsum(dim=0, dtype=torch.int32)
        global_seqlen = self.cu_seqlens_q[-1].item()
        self.ranges_q = AttnRanges.from_cu_seqlens(self.cu_seqlens_q, global_seqlen)
        self.ranges_k = AttnRanges.from_cu_seqlens(self.cu_seqlens_k, global_seqlen)
        self.max_seqlen_q = self.seqlens.max().item()
        self.max_seqlen_k = self.seqlens.max().item()

        self.attention_mask = torch.ones(
            global_seqlen, device=device, dtype=torch.int32
        )

        if self.is_causal:
            self.attn_mask_type = AttnMaskType.CAUSAL
        else:
            self.attn_mask_type = AttnMaskType.FULL


@dataclass
class AttnConfig2D:
    head_size: int
    context_size: int
    slide_window_size: int
    head_first: bool


class Initializer_2D_SEQUENCE_PARALLEL:
    def __init__(
        self,
        head_size,
        head_first,
        context_size,
        window_size,
        world_size,
        rank,
    ):
        self.head_size = head_size
        self.context_size = context_size
        self.window_size = window_size
        self.head_first = head_first
        self.sequence_parallel_size = world_size
        self.interleaved = False
        self.rank = rank

        assert self.context_size * self.head_size == self.sequence_parallel_size

        self.num_head_pgs = self.context_size
        self.num_context_pgs = self.head_size

        self.groups = {
            ParallelMode.HEAD: None,
            ParallelMode.CONTEXT: None,
            ParallelMode.INTRA_WINDOW: None,
            ParallelMode.INTER_WINDOW: None,
            ParallelMode.DKV_INTRA_WINDOW: None,
            ParallelMode.DKV_INTER_WINDOW: None,
        }
        self.cp_ranks = {
            ParallelMode.HEAD: None,
            ParallelMode.CONTEXT: None,
            ParallelMode.INTRA_WINDOW: None,
            ParallelMode.INTER_WINDOW: None,
            ParallelMode.DKV_INTRA_WINDOW: None,
            ParallelMode.DKV_INTER_WINDOW: None,
        }

    def get_sliding_window_pg(self, window_num, context_ranks):
        # context_ranks = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
        # window_size = 4
        # window_num = 4
        # intra_window = [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
        # inter_window = [[0, 4, 8, 12], [1, 5, 9, 13], [2, 6, 10, 14], [3, 7, 11, 15]]

        # create the intra_window process group when using sliding window
        for j in range(window_num):
            intra_window_ranks = context_ranks[
                j * self.window_size : (j + 1) * self.window_size
            ]

            # intra_window
            intra_window_group = dist.new_group(intra_window_ranks)
            if self.rank in intra_window_ranks:
                self.groups[ParallelMode.INTRA_WINDOW] = intra_window_group
                self.cp_ranks[ParallelMode.INTRA_WINDOW] = intra_window_ranks

            # dkv_intra_window
            dkv_intra_window_group = dist.new_group(intra_window_ranks)
            if self.rank in intra_window_ranks:
                self.groups[ParallelMode.DKV_INTRA_WINDOW] = dkv_intra_window_group
                self.cp_ranks[ParallelMode.DKV_INTRA_WINDOW] = intra_window_ranks

        # create the inter_window process group when using sliding window
        for j in range(self.window_size):
            inter_window_ranks = []
            for t in range(window_num):
                inter_window_ranks.append(context_ranks[t * self.window_size + j])

            # inter_window
            inter_window_group = dist.new_group(inter_window_ranks)
            if self.rank in inter_window_ranks:
                self.groups[ParallelMode.INTER_WINDOW] = inter_window_group
                self.cp_ranks[ParallelMode.INTER_WINDOW] = inter_window_ranks

            # dkv_inter_window
            dkv_inter_window_group = dist.new_group(inter_window_ranks)
            if self.rank in inter_window_ranks:
                self.groups[ParallelMode.DKV_INTER_WINDOW] = dkv_inter_window_group
                self.cp_ranks[ParallelMode.DKV_INTER_WINDOW] = inter_window_ranks

    def init_dist_group(self):
        if self.head_first:
            # head process group
            for i in range(self.num_head_pgs):
                head_ranks = list(
                    range(
                        i * self.head_size,
                        (i + 1) * self.head_size,
                    )
                )
                head_pg = dist.new_group(head_ranks)
                if self.rank in head_ranks:
                    self.groups[ParallelMode.HEAD] = head_pg
                    self.cp_ranks[ParallelMode.HEAD] = head_ranks

            for i in range(self.num_context_pgs):
                assert (
                    self.context_size % self.window_size == 0
                ), "the window size should be divided by the context_size."
                window_num = self.context_size // self.window_size

                context_ranks = list(
                    range(i, self.sequence_parallel_size, self.num_context_pgs)
                )
                context_pg = dist.new_group(context_ranks)
                if self.rank in context_ranks:
                    self.groups[ParallelMode.CONTEXT] = context_pg
                    self.cp_ranks[ParallelMode.CONTEXT] = context_ranks
                    self.get_sliding_window_pg(window_num, context_ranks)
        else:
            for i in range(self.num_context_pgs):
                context_ranks = list(
                    range(i * self.context_size, (i + 1) * self.context_size)
                )

                context_pg = dist.new_group(context_ranks)
                if self.rank in context_ranks:
                    self.groups[ParallelMode.CONTEXT] = context_pg
                    self.cp_ranks[ParallelMode.CONTEXT] = context_ranks
                    self.get_sliding_window_pg(window_num, context_ranks)

                assert (
                    self.context_size % self.window_size == 0
                ), "the window size should be divided by the context_size."
                window_num = self.context_size // self.window_size

                # self.get_sliding_window_pg(window_num, context_ranks)

            # head process group
            for i in range(self.num_head_pgs):
                head_ranks = list(
                    range(i, self.sequence_parallel_size, self.num_head_pgs)
                )
                head_pg = dist.new_group(head_ranks)

                if self.rank in head_ranks:
                    self.groups[ParallelMode.HEAD] = head_pg
                    self.cp_ranks[ParallelMode.HEAD] = head_ranks

        return self.groups, self.cp_ranks


class TestTeUlysessAttn(DistTestBase):
    @property
    def process_group(self):
        config_2d = AttnConfig2D(
            head_size=1, context_size=4, slide_window_size=2, head_first=True
        )
        initializer = Initializer_2D_SEQUENCE_PARALLEL(
            head_size=config_2d.head_size,
            head_first=config_2d.head_first,
            context_size=config_2d.context_size,
            window_size=config_2d.slide_window_size,
            world_size=self.world_size,
            rank=self.rank,
        )

        cp_group, cp_ranks = initializer.init_dist_group()
        return cp_group, config_2d

    @property
    def world_size(self) -> int:
        return 4

    @property
    def seed(self) -> int:
        return 42

    def gen_test_data(self, seqlen, nheads, d, device, dtype, test_bwd):
        # Prepare inputs
        q = torch.randn(
            seqlen,
            nheads,
            d,
            device=device,
            dtype=dtype,
            requires_grad=True if test_bwd else False,
        )
        k = torch.randn(
            seqlen,
            nheads,
            d,
            device=device,
            dtype=dtype,
            requires_grad=True if test_bwd else False,
        )
        v = torch.randn(
            seqlen,
            nheads,
            d,
            device=device,
            dtype=dtype,
            requires_grad=True if test_bwd else False,
        )
        dout = torch.randn(seqlen, nheads, d, device=device, dtype=dtype)
        return q, k, v, dout

    def fa_varlen_impl(self, q, k, v, dout, attn_config, attn_arg, test_bwd):
        q0 = q.detach().clone()
        k0 = k.detach().clone()
        v0 = v.detach().clone()
        if test_bwd:
            q0.requires_grad = True
            k0.requires_grad = True
            v0.requires_grad = True
        out_fa, lse_fa, _ = flash_attn_varlen_func(
            q0,
            k0,
            v0,
            attn_arg.cu_seqlens_q,
            attn_arg.cu_seqlens_k,
            attn_config.total_seqlen // 2,
            attn_config.total_seqlen // 2,
            dropout_p=attn_config.dropout_p,
            softmax_scale=None,
            causal=attn_arg.is_causal,
            return_attn_probs=True,
            deterministic=True,
        )
        dist.barrier()
        if test_bwd:
            out_fa.backward(dout)
        return out_fa, lse_fa, q0.grad, k0.grad, v0.grad

    def mytest_loongtrain_dispatch(self, attn_config, attn_arg, rank, device, test_bwd):
        q, k, v, dout = self.gen_test_data(
            attn_config.total_seqlen,
            attn_config.num_heads,
            attn_config.head_dim,
            device=device,
            dtype=attn_config.dtype,
            test_bwd=test_bwd,
        )

        dist.broadcast(q, src=0)
        dist.broadcast(k, src=0)
        dist.broadcast(v, src=0)
        dist.broadcast(dout, src=0)

        loongtrain = LoongTrain()

        world_size = self.world_size
        cp_group, config_2d = self.process_group
        q_local = (
            loongtrain.dispatch(
                q,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="q",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        k_local = (
            loongtrain.dispatch(
                k,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="k",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        v_local = (
            loongtrain.dispatch(
                v,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="v",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )

        # test dispatch undispatch
        q_global = loongtrain.undispatch(
            q_local,
            rank,
            world_size,
            cp_group,
            qkv_="q",
            qkv_format=attn_config.qkv_format,
        )
        k_global = loongtrain.undispatch(
            k_local,
            rank,
            world_size,
            cp_group,
            qkv_="k",
            qkv_format=attn_config.qkv_format,
        )
        v_global = loongtrain.undispatch(
            v_local,
            rank,
            world_size,
            cp_group,
            qkv_="v",
            qkv_format=attn_config.qkv_format,
        )

        assert torch.equal(q, q_global)
        assert torch.equal(k, k_global)
        assert torch.equal(v, v_global)

    def mytest_loongtrain_impl(self, attn_config, attn_arg, rank, device, test_bwd):
        q, k, v, dout = self.gen_test_data(
            attn_config.total_seqlen,
            attn_config.num_heads,
            attn_config.head_dim,
            device=device,
            dtype=attn_config.dtype,
            test_bwd=test_bwd,
        )

        dist.broadcast(q, src=0)
        dist.broadcast(k, src=0)
        dist.broadcast(v, src=0)
        dist.broadcast(dout, src=0)

        loongtrain = LoongTrain()

        world_size = self.world_size
        cp_group, config_2d = self.process_group
        q_local = (
            loongtrain.dispatch(
                q,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="q",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        k_local = (
            loongtrain.dispatch(
                k,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="k",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        v_local = (
            loongtrain.dispatch(
                v,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="v",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        dout_local = (
            loongtrain.dispatch(
                dout,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="dout",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )

        if test_bwd:
            q_local.requires_grad = True
            k_local.requires_grad = True
            v_local.requires_grad = True

        # with torch.cuda.device(device):
        #     cp_stream = torch.cuda.Stream()  # 创建流
        local_out, local_lse = loongtrain.apply_attn(
            q_local,
            k_local,
            v_local,
            attn_arg.ranges_q,
            attn_arg.ranges_k,
            attn_arg.attn_mask_type,
            attn_config.total_seqlen,
            attn_config.total_seqlen,
            None,
            attn_config.deterministic,
            cp_group=cp_group,
            qkv_format=attn_config.qkv_format,
            dropout_p=attn_config.dropout_p,
            slide_window_size=config_2d.slide_window_size,
        )

        if test_bwd:
            local_out.backward(dout_local)
            # grad_q_local = q_local.grad
            # grad_k_local = k_local.grad
            # grad_v_local = v_local.grad

        # total_out_ref = scaled_dot_product_attention(
        #     rearrange(q, "t h d -> 1 h t d"),
        #     rearrange(k, "t h d -> 1 h t d"),
        #     rearrange(v, "t h d -> 1 h t d"),
        #     attn_mask=None,
        #     dropout_p=attn_config.dropout_p,
        #     is_causal=attn_arg.is_causal,
        # )
        # total_out_ref = rearrange(total_out_ref, "1 h t d -> t h d")
        # total_out_ref.backward(dout)

        # local_out_ref = teulysess_func.dispatch(total_out_ref, rank, world_size, cp_group, ranges=attn_arg.ranges_q, attention_mask_thd=attn_arg.attention_mask, qkv_="out", qkv_format = attn_config.qkv_format).detach().clone()    # noqa: E501
        # if test_bwd:
        #     local_grad_q_ref = teulysess_func.dispatch(q.grad, rank, world_size, cp_group, ranges=attn_arg.ranges_q, attention_mask_thd=attn_arg.attention_mask, qkv_="q", qkv_format = attn_config.qkv_format).detach().clone()  # noqa: E501
        #     local_grad_k_ref = teulysess_func.dispatch(k.grad, rank, world_size, cp_group, ranges=attn_arg.ranges_k, attention_mask_thd=attn_arg.attention_mask, qkv_="k", qkv_format = attn_config.qkv_format).detach().clone()  # noqa: E501
        #     local_grad_v_ref = teulysess_func.dispatch(v.grad, rank, world_size, cp_group, ranges=attn_arg.ranges_k, attention_mask_thd=attn_arg.attention_mask, qkv_="v", qkv_format = attn_config.qkv_format).detach().clone()  # noqa: E501

        total_out_ref, fa_lse, fa_dq, fa_dk, fa_dv = self.fa_varlen_impl(
            q, k, v, dout, attn_config, attn_arg, test_bwd
        )

        local_out_ref = (
            loongtrain.dispatch(
                total_out_ref,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="out",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        if test_bwd:
            local_grad_q_ref = (
                loongtrain.dispatch(
                    fa_dq,
                    rank,
                    world_size,
                    cp_group,
                    ranges=attn_arg.ranges_q,
                    attention_mask_thd=attn_arg.attention_mask,
                    qkv_="q",
                    qkv_format=attn_config.qkv_format,
                )
                .detach()
                .clone()
            )
            local_grad_k_ref = (
                loongtrain.dispatch(
                    fa_dk,
                    rank,
                    world_size,
                    cp_group,
                    ranges=attn_arg.ranges_k,
                    attention_mask_thd=attn_arg.attention_mask,
                    qkv_="k",
                    qkv_format=attn_config.qkv_format,
                )
                .detach()
                .clone()
            )
            local_grad_v_ref = (
                loongtrain.dispatch(
                    fa_dv,
                    rank,
                    world_size,
                    cp_group,
                    ranges=attn_arg.ranges_k,
                    attention_mask_thd=attn_arg.attention_mask,
                    qkv_="v",
                    qkv_format=attn_config.qkv_format,
                )
                .detach()
                .clone()
            )

        torch.testing.assert_close(local_out, local_out_ref, atol=1e-2, rtol=1e-2)
        if test_bwd:
            torch.testing.assert_close(
                q_local.grad, local_grad_q_ref, atol=1e-2, rtol=1e-2
            )
            torch.testing.assert_close(
                k_local.grad, local_grad_k_ref, atol=1e-2, rtol=1e-2
            )
            torch.testing.assert_close(
                v_local.grad, local_grad_v_ref, atol=1e-2, rtol=1e-2
            )

    @skip_if_lt_x_gpu(4)
    @with_comms
    def test_loongtrain_attn(self):
        device = torch.cuda.current_device()
        # device = torch.device(f"cuda:{self.rank}")

        attn_config = LoongTrainAttnConfig(
            total_seqlen=4096,
            num_heads=16,
            head_dim=128,
            dtype=torch.float16,
            deterministic=True,
            qkv_format="thd",
            dropout_p=0.0,
        )

        seqlens = torch.tensor(
            [0, attn_config.total_seqlen // 2, attn_config.total_seqlen // 2],
            device=device,
            dtype=torch.int32,
        )
        causal = False
        test_bwd = True

        local_attn_arg = LoongTrainAttnArg(
            seqlens=seqlens,
            is_causal=causal,
        )

        self.mytest_loongtrain_dispatch(
            attn_config, local_attn_arg, self.rank, device, test_bwd
        )
        self.mytest_loongtrain_impl(
            attn_config, local_attn_arg, self.rank, device, test_bwd
        )


if __name__ == "__main__":
    run_tests()
