# Copyright (c) 2025 SandAI. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import datetime
from dataclasses import dataclass
from typing import Optional

import deepspeed
import deepspeed.comm as dist
import torch
from flash_attn import flash_attn_varlen_func
from torch.testing._internal.common_distributed import skip_if_lt_x_gpu
from torch.testing._internal.common_utils import run_tests

from magi_attention.common.enum import AttnMaskType
from magi_attention.common.ranges import AttnRanges
from magi_attention.testing.dist_common import DistTestBase, with_comms

# isort: split
from exps.dist_attn.baselines.ulysses import DSUlysses


@dataclass
class DsUlysessAttnConfig:
    # 总序列长度
    total_seqlen: int
    # 头数
    num_heads: int
    head_dim: int
    dtype: torch.dtype
    deterministic: bool
    qkv_format: str
    dropout_p: float


@dataclass
class DsUlysessAttnArg:
    seqlens: torch.Tensor
    is_causal: bool

    # Users should not set the following variables, as they are automatically generated by __post_init__.
    cu_seqlens_q: Optional[torch.Tensor] = None
    cu_seqlens_k: Optional[torch.Tensor] = None
    max_seqlen_q: Optional[int] = None
    max_seqlen_k: Optional[int] = None
    ranges_q: Optional[AttnRanges] = None
    ranges_k: Optional[AttnRanges] = None
    attn_mask_type: Optional[AttnMaskType] = None
    attention_mask: Optional[torch.Tensor] = None

    def __post_init__(self):
        device = self.seqlens.device
        self.cu_seqlens_q = self.seqlens.cumsum(dim=0, dtype=torch.int32)
        self.cu_seqlens_k = self.seqlens.cumsum(dim=0, dtype=torch.int32)
        global_seqlen = self.cu_seqlens_q[-1].item()
        self.ranges_q = AttnRanges.from_cu_seqlens(self.cu_seqlens_q, global_seqlen)
        self.ranges_k = AttnRanges.from_cu_seqlens(self.cu_seqlens_k, global_seqlen)
        self.max_seqlen_q = self.seqlens.max().item()
        self.max_seqlen_k = self.seqlens.max().item()

        self.attention_mask = torch.ones(
            global_seqlen, device=device, dtype=torch.int32
        )

        if self.is_causal:
            self.attn_mask_type = AttnMaskType.CAUSAL
        else:
            self.attn_mask_type = AttnMaskType.FULL


class TestDsUlysessAttn(DistTestBase):
    @property
    def process_group(self):
        ulysses_ranks = list(range(0, self.world_size))
        group = dist.new_group(ulysses_ranks)
        return group

    @property
    def world_size(self) -> int:
        return 4

    @property
    def seed(self) -> int:
        return 42

    def init_pg(self) -> None:
        if "nccl" in self.backend and torch.cuda.device_count() < self.world_size:
            raise RuntimeError(
                f"nccl backend requires {self.world_size} GPUs, but only {torch.cuda.device_count()} are available"
            )

        if self.backend not in ["nccl", "gloo", "mpi", "cpu:gloo,cuda:nccl"]:
            raise RuntimeError(f"Backend {self.backend} not supported!")

        deepspeed.init_distributed(
            dist_backend=self.backend,
            world_size=self.world_size,
            rank=self.rank,
            init_method=f"file://{self.file_name}",
            timeout=datetime.timedelta(minutes=30),
        )
        # dist.init_process_group(
        #     backend=self.backend,
        #     world_size=self.world_size,
        #     rank=self.rank,
        #     init_method=f"file://{self.file_name}",  # noqa
        #     timeout=datetime.timedelta(minutes=30),
        # )

        # set device for nccl pg for collectives
        if "nccl" in self.backend:
            torch.cuda.set_device(self.rank)

    def gen_test_data(self, seqlen, nheads, d, device, dtype, test_bwd):
        # Prepare inputs
        q = torch.randn(
            seqlen,
            nheads,
            d,
            device=device,
            dtype=dtype,
            requires_grad=True if test_bwd else False,
        )
        k = torch.randn(
            seqlen,
            nheads,
            d,
            device=device,
            dtype=dtype,
            requires_grad=True if test_bwd else False,
        )
        v = torch.randn(
            seqlen,
            nheads,
            d,
            device=device,
            dtype=dtype,
            requires_grad=True if test_bwd else False,
        )
        dout = torch.randn(seqlen, nheads, d, device=device, dtype=dtype)
        return q, k, v, dout

    def fa_varlen_impl(self, q, k, v, dout, attn_config, attn_arg, test_bwd):
        q0 = q.detach().clone()
        k0 = k.detach().clone()
        v0 = v.detach().clone()
        if test_bwd:
            q0.requires_grad = True
            k0.requires_grad = True
            v0.requires_grad = True
        out_fa, lse_fa, _ = flash_attn_varlen_func(
            q0,
            k0,
            v0,
            attn_arg.cu_seqlens_q,
            attn_arg.cu_seqlens_k,
            attn_config.total_seqlen // 2,
            attn_config.total_seqlen // 2,
            dropout_p=attn_config.dropout_p,
            softmax_scale=None,
            causal=attn_arg.is_causal,
            return_attn_probs=True,
            deterministic=True,
        )
        dist.barrier()
        if test_bwd:
            out_fa.backward(dout)
        return out_fa, lse_fa, q0.grad, k0.grad, v0.grad

    def mytest_dsulysess_dispatch(self, attn_config, attn_arg, rank, device, test_bwd):
        q, k, v, dout = self.gen_test_data(
            attn_config.total_seqlen,
            attn_config.num_heads,
            attn_config.head_dim,
            device=device,
            dtype=attn_config.dtype,
            test_bwd=test_bwd,
        )

        dist.broadcast(q, src=0)
        dist.broadcast(k, src=0)
        dist.broadcast(v, src=0)
        dist.broadcast(dout, src=0)

        world_size = self.world_size
        cp_group = self.process_group
        cp_stream = None
        if attn_config.qkv_format == "thd" or attn_config.qkv_format == "bshd":
            dsulysess_func = DSUlysses(cp_group, 2, 1, sp_stream=cp_stream)
        else:
            dsulysess_func = DSUlysses(cp_group, 2, 0, sp_stream=cp_stream)

        world_size = self.world_size
        cp_group = self.process_group
        q_local = (
            dsulysess_func.dispatch(
                q,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="q",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        k_local = (
            dsulysess_func.dispatch(
                k,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="k",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        v_local = (
            dsulysess_func.dispatch(
                v,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="v",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )

        # test dispatch undispatch
        q_global = dsulysess_func.undispatch(
            q_local,
            rank,
            world_size,
            cp_group,
            qkv_="q",
            qkv_format=attn_config.qkv_format,
        )
        k_global = dsulysess_func.undispatch(
            k_local,
            rank,
            world_size,
            cp_group,
            qkv_="k",
            qkv_format=attn_config.qkv_format,
        )
        v_global = dsulysess_func.undispatch(
            v_local,
            rank,
            world_size,
            cp_group,
            qkv_="v",
            qkv_format=attn_config.qkv_format,
        )

        assert torch.equal(q, q_global)
        assert torch.equal(k, k_global)
        assert torch.equal(v, v_global)

    def mytest_dsulysess_impl(self, attn_config, attn_arg, rank, device, test_bwd):
        q, k, v, dout = self.gen_test_data(
            attn_config.total_seqlen,
            attn_config.num_heads,
            attn_config.head_dim,
            device=device,
            dtype=attn_config.dtype,
            test_bwd=test_bwd,
        )

        dist.broadcast(q, src=0)
        dist.broadcast(k, src=0)
        dist.broadcast(v, src=0)
        dist.broadcast(dout, src=0)

        world_size = self.world_size
        cp_group = self.process_group
        cp_stream = None
        if attn_config.qkv_format == "thd" or attn_config.qkv_format == "bshd":
            dsulysess_func = DSUlysses(cp_group, 2, 1, sp_stream=cp_stream)
        else:
            dsulysess_func = DSUlysses(cp_group, 2, 0, sp_stream=cp_stream)

        q_local = (
            dsulysess_func.dispatch(
                q,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="q",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        k_local = (
            dsulysess_func.dispatch(
                k,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="k",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        v_local = (
            dsulysess_func.dispatch(
                v,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="v",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        dout_local = (
            dsulysess_func.dispatch(
                dout,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="dout",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )

        if test_bwd:
            q_local.requires_grad = True
            k_local.requires_grad = True
            v_local.requires_grad = True

        local_out, local_lse = dsulysess_func.apply_attn(
            q_local,
            k_local,
            v_local,
            attn_arg.ranges_q,
            attn_arg.ranges_k,
            attn_arg.attn_mask_type,
            attn_config.total_seqlen,
            attn_config.total_seqlen,
            None,
            attn_config.deterministic,
            cp_group=cp_group,
            qkv_format=attn_config.qkv_format,
            dropout_p=attn_config.dropout_p,
            cp_stream=cp_stream,
            use_fused_attention=True,
        )

        if test_bwd:
            local_out.backward(dout_local)
            # grad_q_local = q_local.grad
            # grad_k_local = k_local.grad
            # grad_v_local = v_local.grad

        # total_out_ref = scaled_dot_product_attention(
        #     rearrange(q, "t h d -> 1 h t d"),
        #     rearrange(k, "t h d -> 1 h t d"),
        #     rearrange(v, "t h d -> 1 h t d"),
        #     attn_mask=None,
        #     dropout_p=attn_config.dropout_p,
        #     is_causal=attn_arg.is_causal,
        # )
        # total_out_ref = rearrange(total_out_ref, "1 h t d -> t h d")
        # total_out_ref.backward(dout)

        # local_out_ref = teulysess_func.dispatch(total_out_ref, rank, world_size, cp_group, ranges=attn_arg.ranges_q, attention_mask_thd=attn_arg.attention_mask, qkv_="out", qkv_format = attn_config.qkv_format).detach().clone()    # noqa: E501
        # if test_bwd:
        #     local_grad_q_ref = teulysess_func.dispatch(q.grad, rank, world_size, cp_group, ranges=attn_arg.ranges_q, attention_mask_thd=attn_arg.attention_mask, qkv_="q", qkv_format = attn_config.qkv_format).detach().clone()  # noqa: E501
        #     local_grad_k_ref = teulysess_func.dispatch(k.grad, rank, world_size, cp_group, ranges=attn_arg.ranges_k, attention_mask_thd=attn_arg.attention_mask, qkv_="k", qkv_format = attn_config.qkv_format).detach().clone()  # noqa: E501
        #     local_grad_v_ref = teulysess_func.dispatch(v.grad, rank, world_size, cp_group, ranges=attn_arg.ranges_k, attention_mask_thd=attn_arg.attention_mask, qkv_="v", qkv_format = attn_config.qkv_format).detach().clone()  # noqa: E501

        total_out_ref, fa_lse, fa_dq, fa_dk, fa_dv = self.fa_varlen_impl(
            q, k, v, dout, attn_config, attn_arg, test_bwd
        )

        local_out_ref = (
            dsulysess_func.dispatch(
                total_out_ref,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="out",
                qkv_format=attn_config.qkv_format,
            )
            .detach()
            .clone()
        )
        if test_bwd:
            local_grad_q_ref = (
                dsulysess_func.dispatch(
                    fa_dq,
                    rank,
                    world_size,
                    cp_group,
                    ranges=attn_arg.ranges_q,
                    attention_mask_thd=attn_arg.attention_mask,
                    qkv_="q",
                    qkv_format=attn_config.qkv_format,
                )
                .detach()
                .clone()
            )
            local_grad_k_ref = (
                dsulysess_func.dispatch(
                    fa_dk,
                    rank,
                    world_size,
                    cp_group,
                    ranges=attn_arg.ranges_k,
                    attention_mask_thd=attn_arg.attention_mask,
                    qkv_="k",
                    qkv_format=attn_config.qkv_format,
                )
                .detach()
                .clone()
            )
            local_grad_v_ref = (
                dsulysess_func.dispatch(
                    fa_dv,
                    rank,
                    world_size,
                    cp_group,
                    ranges=attn_arg.ranges_k,
                    attention_mask_thd=attn_arg.attention_mask,
                    qkv_="v",
                    qkv_format=attn_config.qkv_format,
                )
                .detach()
                .clone()
            )

        torch.testing.assert_close(local_out, local_out_ref, atol=1e-2, rtol=1e-2)
        if test_bwd:
            torch.testing.assert_close(
                q_local.grad, local_grad_q_ref, atol=1e-2, rtol=1e-2
            )
            torch.testing.assert_close(
                k_local.grad, local_grad_k_ref, atol=1e-2, rtol=1e-2
            )
            torch.testing.assert_close(
                v_local.grad, local_grad_v_ref, atol=1e-2, rtol=1e-2
            )

    @skip_if_lt_x_gpu(4)
    @with_comms
    def test_dsulysess_attn(self):
        device = torch.cuda.current_device()
        # device = torch.device(f"cuda:{self.rank}")

        attn_config = DsUlysessAttnConfig(
            total_seqlen=4096,
            num_heads=16,
            head_dim=128,
            dtype=torch.float16,
            deterministic=True,
            qkv_format="thd",
            dropout_p=0.0,
        )

        seqlens = torch.tensor(
            [0, attn_config.total_seqlen // 2, attn_config.total_seqlen // 2],
            device=device,
            dtype=torch.int32,
        )
        causal = True
        test_bwd = True

        local_attn_arg = DsUlysessAttnArg(
            seqlens=seqlens,
            is_causal=causal,
        )

        self.mytest_dsulysess_dispatch(
            attn_config, local_attn_arg, self.rank, device, test_bwd
        )
        self.mytest_dsulysess_impl(
            attn_config, local_attn_arg, self.rank, device, test_bwd
        )


if __name__ == "__main__":
    run_tests()
