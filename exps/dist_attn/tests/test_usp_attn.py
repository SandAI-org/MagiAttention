# Copyright (c) 2025 SandAI. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass
from typing import Optional

import torch
import torch.distributed
import torch.distributed as dist
from flash_attn import flash_attn_varlen_func
from torch.testing._internal.common_distributed import skip_if_lt_x_gpu
from torch.testing._internal.common_utils import run_tests

from magi_attention.common.enum import AttnMaskType
from magi_attention.common.ranges import AttnRanges
from magi_attention.testing.dist_common import DistTestBase, with_comms

# isort: split
from exps.dist_attn.baselines.usp import TEUSP


@dataclass
class TeUSPAttnConfig:
    # 总序列长度
    total_seqlen: int
    # 头数
    num_heads: int
    head_dim: int
    dtype: torch.dtype
    deterministic: bool
    qkv_format: str
    dropout_p: float


@dataclass
class TeUSPAttnArg:
    seqlens: torch.Tensor
    is_causal: bool

    # Users should not set the following variables, as they are automatically generated by __post_init__.
    cu_seqlens_q: Optional[torch.Tensor] = None
    cu_seqlens_k: Optional[torch.Tensor] = None
    max_seqlen_q: Optional[int] = None
    max_seqlen_k: Optional[int] = None
    ranges_q: Optional[AttnRanges] = None
    ranges_k: Optional[AttnRanges] = None
    attn_mask_type: Optional[AttnMaskType] = None
    attention_mask: Optional[torch.Tensor] = None

    def __post_init__(self):
        device = self.seqlens.device
        self.cu_seqlens_q = self.seqlens.cumsum(dim=0, dtype=torch.int32)
        self.cu_seqlens_k = self.seqlens.cumsum(dim=0, dtype=torch.int32)
        global_seqlen = self.cu_seqlens_q[-1].item()
        self.ranges_q = AttnRanges.from_cu_seqlens(self.cu_seqlens_q, global_seqlen)
        self.ranges_k = AttnRanges.from_cu_seqlens(self.cu_seqlens_k, global_seqlen)
        self.max_seqlen_q = self.seqlens.max().item()
        self.max_seqlen_k = self.seqlens.max().item()

        self.attention_mask = torch.ones(
            global_seqlen, device=device, dtype=torch.int32
        )

        if self.is_causal:
            self.attn_mask_type = AttnMaskType.CAUSAL
        else:
            self.attn_mask_type = AttnMaskType.FULL


class TestTeUSPAttn(DistTestBase):
    @property
    def process_group(self):
        """
        sp_ulysses_degree x sp_ring_degree = seq_parallel_degree
        (ulysses_degree, dp_degree)
        """

        world_size = self.world_size
        use_ulysses_low = self.use_ulysess_low
        sp_ulysses_degree = self.ulysses_degree
        sp_ring_degree = world_size // sp_ulysses_degree

        num_ulysses_pgs = sp_ring_degree  # world_size // sp_ulysses_degree
        num_ring_pgs = sp_ulysses_degree  # world_size // sp_ring_degree

        ulyssess_pg, ring_pg = None, None
        if use_ulysses_low:
            for i in range(num_ulysses_pgs):
                ulysses_ranks = list(
                    range(
                        i * sp_ulysses_degree,
                        (i + 1) * sp_ulysses_degree,
                    )
                )
                group = torch.distributed.new_group(ulysses_ranks)
                if self.rank in ulysses_ranks:
                    ulyssess_pg = group

            for i in range(num_ring_pgs):
                ring_ranks = list(range(i, world_size, num_ring_pgs))
                group = torch.distributed.new_group(ring_ranks)
                if self.rank in ring_ranks:
                    ring_pg = group

        else:
            for i in range(num_ring_pgs):
                ring_ranks = list(range(i * sp_ring_degree, (i + 1) * sp_ring_degree))
                group = torch.distributed.new_group(ring_ranks)
                if self.rank in ring_ranks:
                    ring_pg = group

            for i in range(num_ulysses_pgs):
                ulysses_ranks = list(range(i, world_size, num_ulysses_pgs))
                group = torch.distributed.new_group(ulysses_ranks)
                if self.rank in ulysses_ranks:
                    ulyssess_pg = group

        return ulyssess_pg, ring_pg

    @property
    def world_size(self) -> int:
        return 4

    @property
    def ulysses_degree(self) -> int:
        return 2

    @property
    def use_ulysess_low(self) -> bool:
        return True

    @property
    def seed(self) -> int:
        return 42

    def gen_test_data(self, seqlen, nheads, d, device, dtype, test_bwd):
        # Prepare inputs
        q = torch.randn(
            seqlen,
            nheads,
            d,
            device=device,
            dtype=dtype,
            requires_grad=True if test_bwd else False,
        )
        k = torch.randn(
            seqlen,
            nheads,
            d,
            device=device,
            dtype=dtype,
            requires_grad=True if test_bwd else False,
        )
        v = torch.randn(
            seqlen,
            nheads,
            d,
            device=device,
            dtype=dtype,
            requires_grad=True if test_bwd else False,
        )
        dout = torch.randn(seqlen, nheads, d, device=device, dtype=dtype)
        return q, k, v, dout

    def fa_varlen_impl(self, q, k, v, dout, attn_config, attn_arg, test_bwd):
        q0 = q.detach().clone()
        k0 = k.detach().clone()
        v0 = v.detach().clone()
        if test_bwd:
            q0.requires_grad = True
            k0.requires_grad = True
            v0.requires_grad = True
        out_fa, lse_fa, _ = flash_attn_varlen_func(
            q0,
            k0,
            v0,
            attn_arg.cu_seqlens_q,
            attn_arg.cu_seqlens_k,
            attn_config.total_seqlen // 2,
            attn_config.total_seqlen // 2,
            dropout_p=attn_config.dropout_p,
            softmax_scale=None,
            causal=attn_arg.is_causal,
            return_attn_probs=True,
            deterministic=True,
        )
        dist.barrier()
        if test_bwd:
            out_fa.backward(dout)
        return out_fa, lse_fa, q0.grad, k0.grad, v0.grad

    def mytest_usp_dispatch(self, attn_config, attn_arg, rank, device, test_bwd):
        q, k, v, dout = self.gen_test_data(
            attn_config.total_seqlen,
            attn_config.num_heads,
            attn_config.head_dim,
            device=device,
            dtype=attn_config.dtype,
            test_bwd=test_bwd,
        )

        dist.broadcast(q, src=0)
        dist.broadcast(k, src=0)
        dist.broadcast(v, src=0)
        dist.broadcast(dout, src=0)

        world_size = self.world_size
        ulysess_gp, ring_pg = self.process_group
        if self.use_ulysess_low:
            cp_group = [ulysess_gp, ring_pg]
        else:
            cp_group = [ring_pg, ulysess_gp]

        teusp_func = TEUSP()

        q_local = (
            teusp_func.dispatch(
                q,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="q",
                qkv_format=attn_config.qkv_format,
                use_ulysess_low=self.use_ulysess_low,
            )
            .detach()
            .clone()
        )
        k_local = (
            teusp_func.dispatch(
                k,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="k",
                qkv_format=attn_config.qkv_format,
                use_ulysess_low=self.use_ulysess_low,
            )
            .detach()
            .clone()
        )
        v_local = (
            teusp_func.dispatch(
                v,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="v",
                qkv_format=attn_config.qkv_format,
                use_ulysess_low=self.use_ulysess_low,
            )
            .detach()
            .clone()
        )

        # test dispatch undispatch
        q_global = teusp_func.undispatch(
            q_local,
            rank,
            world_size,
            cp_group,
            qkv_="q",
            qkv_format=attn_config.qkv_format,
            use_ulysess_low=self.use_ulysess_low,
        )
        k_global = teusp_func.undispatch(
            k_local,
            rank,
            world_size,
            cp_group,
            qkv_="k",
            qkv_format=attn_config.qkv_format,
            use_ulysess_low=self.use_ulysess_low,
        )
        v_global = teusp_func.undispatch(
            v_local,
            rank,
            world_size,
            cp_group,
            qkv_="v",
            qkv_format=attn_config.qkv_format,
            use_ulysess_low=self.use_ulysess_low,
        )

        assert torch.equal(q, q_global)
        assert torch.equal(k, k_global)
        assert torch.equal(v, v_global)

    def mytest_usp_impl(self, attn_config, attn_arg, rank, device, test_bwd):
        q, k, v, dout = self.gen_test_data(
            attn_config.total_seqlen,
            attn_config.num_heads,
            attn_config.head_dim,
            device=device,
            dtype=attn_config.dtype,
            test_bwd=test_bwd,
        )

        dist.broadcast(q, src=0)
        dist.broadcast(k, src=0)
        dist.broadcast(v, src=0)
        dist.broadcast(dout, src=0)

        world_size = self.world_size
        ulysess_gp, ring_pg = self.process_group
        if self.use_ulysess_low:
            cp_group = [ulysess_gp, ring_pg]
        else:
            cp_group = [ring_pg, ulysess_gp]

        cp_global_ranks = [i for i in range(self.world_size)]

        with torch.cuda.device(device):
            cp_stream = torch.cuda.Stream()  # 创建流
        teusp_func = TEUSP()

        q_local = (
            teusp_func.dispatch(
                q,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="q",
                qkv_format=attn_config.qkv_format,
                use_ulysess_low=self.use_ulysess_low,
            )
            .detach()
            .clone()
        )
        k_local = (
            teusp_func.dispatch(
                k,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="k",
                qkv_format=attn_config.qkv_format,
                use_ulysess_low=self.use_ulysess_low,
            )
            .detach()
            .clone()
        )
        v_local = (
            teusp_func.dispatch(
                v,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_k,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="v",
                qkv_format=attn_config.qkv_format,
                use_ulysess_low=self.use_ulysess_low,
            )
            .detach()
            .clone()
        )
        dout_local = (
            teusp_func.dispatch(
                dout,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="dout",
                qkv_format=attn_config.qkv_format,
                use_ulysess_low=self.use_ulysess_low,
            )
            .detach()
            .clone()
        )

        if test_bwd:
            q_local.requires_grad = True
            k_local.requires_grad = True
            v_local.requires_grad = True

        local_out, local_lse = teusp_func.apply_attn(
            q_local,
            k_local,
            v_local,
            attn_arg.ranges_q,
            attn_arg.ranges_k,
            attn_arg.attn_mask_type,
            attn_config.total_seqlen,
            attn_config.total_seqlen,
            None,
            attn_config.deterministic,
            cp_group=cp_group,
            qkv_format=attn_config.qkv_format,
            dropout_p=attn_config.dropout_p,
            cp_stream=cp_stream,
            use_fused_attention=False,
            use_sync=False,
            cp_global_ranks=cp_global_ranks,
        )

        if test_bwd:
            local_out.backward(dout_local)
            # grad_q_local = q_local.grad
            # grad_k_local = k_local.grad
            # grad_v_local = v_local.grad

        # total_out_ref = scaled_dot_product_attention(
        #     rearrange(q, "t h d -> 1 h t d"),
        #     rearrange(k, "t h d -> 1 h t d"),
        #     rearrange(v, "t h d -> 1 h t d"),
        #     attn_mask=None,
        #     dropout_p=attn_config.dropout_p,
        #     is_causal=attn_arg.is_causal,
        # )
        # total_out_ref = rearrange(total_out_ref, "1 h t d -> t h d")
        # total_out_ref.backward(dout)

        # local_out_ref = teulysess_func.dispatch(total_out_ref, rank, world_size, cp_group, ranges=attn_arg.ranges_q, attention_mask_thd=attn_arg.attention_mask, qkv_="out", qkv_format = attn_config.qkv_format).detach().clone()    # noqa: E501
        # if test_bwd:
        #     local_grad_q_ref = teulysess_func.dispatch(q.grad, rank, world_size, cp_group, ranges=attn_arg.ranges_q, attention_mask_thd=attn_arg.attention_mask, qkv_="q", qkv_format = attn_config.qkv_format).detach().clone()  # noqa: E501
        #     local_grad_k_ref = teulysess_func.dispatch(k.grad, rank, world_size, cp_group, ranges=attn_arg.ranges_k, attention_mask_thd=attn_arg.attention_mask, qkv_="k", qkv_format = attn_config.qkv_format).detach().clone()  # noqa: E501
        #     local_grad_v_ref = teulysess_func.dispatch(v.grad, rank, world_size, cp_group, ranges=attn_arg.ranges_k, attention_mask_thd=attn_arg.attention_mask, qkv_="v", qkv_format = attn_config.qkv_format).detach().clone()  # noqa: E501

        total_out_ref, fa_lse, fa_dq, fa_dk, fa_dv = self.fa_varlen_impl(
            q, k, v, dout, attn_config, attn_arg, test_bwd
        )

        local_out_ref = (
            teusp_func.dispatch(
                total_out_ref,
                rank,
                world_size,
                cp_group,
                ranges=attn_arg.ranges_q,
                attention_mask_thd=attn_arg.attention_mask,
                qkv_="out",
                qkv_format=attn_config.qkv_format,
                use_ulysess_low=self.use_ulysess_low,
            )
            .detach()
            .clone()
        )
        if test_bwd:
            local_grad_q_ref = (
                teusp_func.dispatch(
                    fa_dq,
                    rank,
                    world_size,
                    cp_group,
                    ranges=attn_arg.ranges_q,
                    attention_mask_thd=attn_arg.attention_mask,
                    qkv_="q",
                    qkv_format=attn_config.qkv_format,
                    use_ulysess_low=self.use_ulysess_low,
                )
                .detach()
                .clone()
            )
            local_grad_k_ref = (
                teusp_func.dispatch(
                    fa_dk,
                    rank,
                    world_size,
                    cp_group,
                    ranges=attn_arg.ranges_k,
                    attention_mask_thd=attn_arg.attention_mask,
                    qkv_="k",
                    qkv_format=attn_config.qkv_format,
                    use_ulysess_low=self.use_ulysess_low,
                )
                .detach()
                .clone()
            )
            local_grad_v_ref = (
                teusp_func.dispatch(
                    fa_dv,
                    rank,
                    world_size,
                    cp_group,
                    ranges=attn_arg.ranges_k,
                    attention_mask_thd=attn_arg.attention_mask,
                    qkv_="v",
                    qkv_format=attn_config.qkv_format,
                    use_ulysess_low=self.use_ulysess_low,
                )
                .detach()
                .clone()
            )

        torch.testing.assert_close(local_out, local_out_ref, atol=1e-2, rtol=1e-2)
        if test_bwd:
            torch.testing.assert_close(
                q_local.grad, local_grad_q_ref, atol=1e-2, rtol=1e-2
            )
            torch.testing.assert_close(
                k_local.grad, local_grad_k_ref, atol=1e-2, rtol=1e-2
            )
            torch.testing.assert_close(
                v_local.grad, local_grad_v_ref, atol=1e-2, rtol=1e-2
            )

    @skip_if_lt_x_gpu(4)
    @with_comms
    def test_usp_attn(self):
        device = torch.cuda.current_device()
        # device = torch.device(f"cuda:{self.rank}")

        attn_config = TeUSPAttnConfig(
            total_seqlen=4096,
            num_heads=16,
            head_dim=128,
            dtype=torch.float16,
            deterministic=True,
            qkv_format="thd",
            dropout_p=0.0,
        )

        seqlens = torch.tensor(
            [0, attn_config.total_seqlen // 2, attn_config.total_seqlen // 2],
            device=device,
            dtype=torch.int32,
        )
        causal = True
        test_bwd = True

        local_attn_arg = TeUSPAttnArg(
            seqlens=seqlens,
            is_causal=causal,
        )

        self.mytest_usp_dispatch(
            attn_config, local_attn_arg, self.rank, device, test_bwd
        )
        self.mytest_usp_impl(attn_config, local_attn_arg, self.rank, device, test_bwd)


if __name__ == "__main__":
    run_tests()
