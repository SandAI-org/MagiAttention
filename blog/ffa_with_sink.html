<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> FFA with Attention Sink | MagiAttention Blog </title> <meta name="author" content="SandAI "> <meta name="description" content="Integrating Flex-Flash-Attention with Attention Sink"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/MagiAttention/blog/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/MagiAttention/blog/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/MagiAttention/blog/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/MagiAttention/blog/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/MagiAttention/blog/assets/img/magiattn/logo/magi-icon.png?84cb1ceb0db8400437ea9eadfbb795f3"> <link rel="stylesheet" href="/MagiAttention/blog/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sandai-org.github.io/MagiAttention/blog/ffa_with_sink"> <script src="/MagiAttention/blog/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/MagiAttention/blog/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/MagiAttention/blog/assets/js/distillpub/template.v2.js"></script> <script src="/MagiAttention/blog/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "FFA with Attention Sink",
            "description": "Integrating Flex-Flash-Attention with Attention Sink",
            "published": "November 17, 2025",
            "authors": [
              
              {
                "author": "Yunpeng Huang",
                "authorURL": "",
                "authorEmail": "yunpenghuang@sand.ai",
                "affiliations": [
                  {
                    "name": "SandAI",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/MagiAttention/blog//"> MagiAttention Blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/MagiAttention/blog/">MagiAttention </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="layout"> <div class="sidebar"> <h3>Contents</h3> <ul> <li> <a href="#introduction">Introduction</a> </li> <li> <a href="#overview">Overview</a> </li> <li> <a href="#user-interface">User Interface</a> </li> <li> <sub> <a href="#ffa-api">FFA API</a> </sub> </li> <li> <sub> <a href="#magiattn-api">MagiAttn API</a> </sub> </li> <li> <sub> <a href="#flash-attention-extension">Flash-Attention Extension</a> </sub> </li> <li> <a href="#math-derivation">Math Derivation</a> </li> <li> <sub> <a href="#ffa-forward">FFA Forward</a> </sub> </li> <li> <sub> <a href="#ffa-backward">FFA Backward</a> </sub> </li> <li> <a href="#implementations">Implementations</a> </li> <li> <sub> <a href="#torch-reference">Torch Reference</a> </sub> </li> <li> <sub> <a href="#ffa-impl">FFA Impl</a> </sub> </li> <li> <sub> <a href="#magiattn-impl">MagiAttn Impl</a> </sub> </li> <li> <a href="#citation">Citation</a> </li> <li> <a href="#references">References</a> </li> </ul> </div> <div class="content"> <div class="post distill"> <d-title> <h1>FFA with Attention Sink</h1> <p>Integrating Flex-Flash-Attention with Attention Sink</p> </d-title> <d-byline></d-byline> <d-article> <h2 id="introduction">Introduction</h2> <p>Large-Scaled Models (LMs) assign significant attention to few tokens (<em>such as the intial tokens in the sequence</em>), even if they are not semantically important, which is known as <b>attention sink</b><d-cite key="xiao2024efficientstreaminglanguagemodels,kang2025toldvisualattentionsink"></d-cite>. Researchers attribute this interesting phenomenon to the nature of $softmax$, which requires attention scores of each query token to always sum up to $1$ for all key tokens in the context, even when some query token does not strongly attend to any key token at all<d-cite key="gu2025attentionsinkemergeslanguage"></d-cite>. Therefore, during the training, we can deliberately add some <u><em>learnable sink tokens</em></u> to the key sequence for each query token to collect those unneeded attention scores to relax the <em>“sum-up-to-one”</em> constraint, which can be seen as a learnable version of $\textit{off-by-one}\space softmax$<d-cite key="miller2025attentionmisc"></d-cite>.</p> <p>However, since sink tokens only affect the $softmax$ operation during the attention forward/backward passes w.r.t. the GPT-OSS implementation<d-cite key="openaiGPT-OSScode-misc"></d-cite>, <b>it is non-trivial to apply learnable attention sink with the (distributed) attention implementations in the style of <u>Flash Attention</u></b><d-cite key="dao2022flashattention,dao2023flashattention,shah2024flashattention3fastaccurateattention"></d-cite>, particularly our own kernel implemenation of <u>Flex-Flash-Attention</u>, as well as the distributed implementation of <u>MagiAttention</u><d-cite key="magiattention2025"></d-cite>.</p> <h2 id="overview">Overview</h2> <p>With the release of <a href="https://github.com/SandAI-org/MagiAttention/releases/tag/v1.0.5" rel="external nofollow noopener" target="_blank">MagiAttention-v1.0.5</a>, we have not only <b>supported the learnable attention sink mechanism</b> for our own kernel / distributed implementations of <u>Flex-Flash-Attention</u> / <u>MagiAttention</u> respectively, but also <b>provided the <em>plug-and-play</em> implementations</b> to integrate the original <u>Flash Attention</u> 2/3 interface<d-cite key="daoFlashAttnInterfaceMisc,daoFlashAttnInterfaceHopperMisc"></d-cite> with attention sink, as one of the <a href="https://github.com/SandAI-org/MagiAttention/tree/main/extensions#flashattention-with-attention-sink" rel="external nofollow noopener" target="_blank">MagiAttention Extensions</a>.</p> <p>In this blog, we will share our own methods about how to integrate the attention implementations in the Flash-Attention style with the learnable attention sink mechanism, including:</p> <ul> <li>the <a href="#user-interface">User Interface</a> update for <a href="#ffa-api">Flex-Flash-Attention</a>, <a href="#magiattn-api">MagiAttention</a> and <a href="#flash-attention-extension">Flash-Attention Extension</a>.</li> <li>the <a href="#math-derivation">Math Derivation</a> of applying the attention sink in both <a href="#ffa-forward">forward</a> and <a href="#ffa-backward">backward</a> passes of Flex-Flash-Attention.</li> <li>the <a href="#implementations">Implementations</a> of the (distributed) learnable attention sink mechanism for <a href="#ffa-impl">Flex-Flash-Attention</a> and <a href="#magiattn-impl">MagiAttention</a>, as well as the naive <a href="#torch-reference">Torch Reference</a>.</li> </ul> <h2 id="user-interface">User Interface</h2> <p>Below, we show the minor update of the user interfaces to support learnable attention sink mechanism for original Flex-Flash-Attention, MagiAttention, as well as the Flash-Attention 2/3 as one of the <a href="https://github.com/SandAI-org/MagiAttention/tree/main/extensions#flashattention-with-attention-sink" rel="external nofollow noopener" target="_blank">MagiAttention Extensions</a>.</p> <h3 id="ffa-api">FFA API</h3> <ul> <li>Just add an optional tensor <code class="language-plaintext highlighter-rouge">sink</code> to the argument list of <code class="language-plaintext highlighter-rouge">flex_flash_attn_func</code>.</li> <li>And when and only when <code class="language-plaintext highlighter-rouge">sink</code> tensor is given, <code class="language-plaintext highlighter-rouge">flex_flash_attn_func</code> will apply attention sink during the forward pass, and compute <code class="language-plaintext highlighter-rouge">dsink</code> (<em>the gradient of <code class="language-plaintext highlighter-rouge">sink</code></em>) during the backward pass.</li> <li>Otherwise, attention sink is skipped and <code class="language-plaintext highlighter-rouge">dsink</code> is also returned as <code class="language-plaintext highlighter-rouge">None</code>.</li> <li>dtype: <code class="language-plaintext highlighter-rouge">float32</code> only.</li> <li>shape: <code class="language-plaintext highlighter-rouge">[seqlen_sink, num_heads_q]</code>, where <code class="language-plaintext highlighter-rouge">seqlen_sink</code> in <code class="language-plaintext highlighter-rouge">[1, 8]</code>.</li> <li>interface difference with the original <code class="language-plaintext highlighter-rouge">flex_flash_attn_func</code>:</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">def flex_flash_attn_func(
</span>    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    q_ranges: torch.Tensor,
    k_ranges: torch.Tensor,
    attn_type_map: torch.Tensor | None = None,
<span class="gi">+   sink: torch.Tensor | None = None,
</span>    softmax_scale: float | None = None,
    softcap: float = 0.0,
    deterministic: bool = False,
    sm_margin: int = 0,
    disable_fwd_atomic_reduction: bool = False,
    auto_range_merge: bool = False,
    ref_block_size: tuple[int, int] | None = None,
    profile_mode: bool = False,
) -&gt; tuple[torch.Tensor, torch.Tensor]:
    ...
</code></pre></div></div> <h3 id="magiattn-api">MagiAttn API</h3> <ul> <li>Just add an optional <strong>replicated</strong> tensor <code class="language-plaintext highlighter-rouge">sink</code> to the argument list of <code class="language-plaintext highlighter-rouge">calc_attn</code>.</li> <li>And when and only when <strong>replicated</strong> <code class="language-plaintext highlighter-rouge">sink</code> tensor is given, <code class="language-plaintext highlighter-rouge">calc_attn</code> will apply attention sink during the forward pass for each <strong>local</strong> query token, and compute <strong>partial</strong> <code class="language-plaintext highlighter-rouge">dsink</code> during the backward pass.</li> <li>And an <code class="language-plaintext highlighter-rouge">all-reduce</code> communication might be applied across cp ranks to return the <strong>reduced</strong> <code class="language-plaintext highlighter-rouge">dsink</code> if required (<em>see the environment variable <code class="language-plaintext highlighter-rouge">MAGI_ATTENTION_DSINK_ALL_REDUCE_OP</code> in our <a href="https://sandai-org.github.io/MagiAttention/docs/main/env_variables.html#for-correctness" rel="external nofollow noopener" target="_blank">docs</a></em>).</li> <li>Otherwise, attention sink is skipped and <code class="language-plaintext highlighter-rouge">dsink</code> is also returned as <code class="language-plaintext highlighter-rouge">None</code>.</li> <li>dtype: <code class="language-plaintext highlighter-rouge">float32</code> only.</li> <li>shape: <code class="language-plaintext highlighter-rouge">[seqlen_sink, num_heads_q]</code>, where <code class="language-plaintext highlighter-rouge">seqlen_sink</code> in <code class="language-plaintext highlighter-rouge">[1, 8]</code>.</li> <li>parallel style: <code class="language-plaintext highlighter-rouge">Replicate</code>.</li> <li>interface difference with the original <code class="language-plaintext highlighter-rouge">calc_attn</code>:</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">def calc_attn(
</span>    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    key: DistAttnRuntimeKey,
<span class="gi">+   sink: torch.Tensor | None = None,
</span>    softmax_scale: float | None = None,
    softcap: float = 0.0,
) -&gt; tuple[torch.Tensor, torch.Tensor]:
    ...
</code></pre></div></div> <h3 id="flash-attention-extension">Flash Attention Extension</h3> <ul> <li>Just add an optional tensor <code class="language-plaintext highlighter-rouge">sink</code> to the argument list of <code class="language-plaintext highlighter-rouge">flash_attn_func</code>, <code class="language-plaintext highlighter-rouge">flash_attn_varlen_func</code>, etc.</li> <li>And when and only when <code class="language-plaintext highlighter-rouge">sink</code> tensor is given, flash attention will apply attention sink during the forward pass, and compute <code class="language-plaintext highlighter-rouge">dsink</code> during the backward pass.</li> <li>Otherwise, attention sink is skipped and <code class="language-plaintext highlighter-rouge">dsink</code> is also returned as <code class="language-plaintext highlighter-rouge">None</code>.</li> <li>dtype: <code class="language-plaintext highlighter-rouge">float32</code> only.</li> <li>shape: <code class="language-plaintext highlighter-rouge">[seqlen_sink, num_heads_q]</code>, where <code class="language-plaintext highlighter-rouge">seqlen_sink</code> has no limit.</li> <li>interface difference with the original flash attention:</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gd">- def flash_attn_func(
</span><span class="gi">+ def flash_attn_func_with_sink(
</span>    q,
    k,
    v,
<span class="gi">+   sink=None,
</span>    softmax_scale=None,
    causal=False,
    qv=None,
    q_descale=None,
    k_descale=None,
    v_descale=None,
    window_size=(-1, -1),
    attention_chunk=0,
    softcap=0.0,
    num_splits=1,
    pack_gqa=None,
    deterministic=False,
    sm_margin=0,
    return_attn_probs=False,
):
    ...
<span class="err">
</span><span class="gd">- def flash_attn_varlen_func(
</span><span class="gi">+ def flash_attn_varlen_func_with_sink(
</span>    q,
    k,
    v,
    cu_seqlens_q,
    cu_seqlens_k,
    max_seqlen_q,
    max_seqlen_k,
<span class="gi">+   sink=None,
</span>    seqused_q=None,
    seqused_k=None,
    softmax_scale=None,
    causal=False,
    qv=None,
    q_descale=None,
    k_descale=None,
    v_descale=None,
    window_size=(-1, -1),
    attention_chunk=0,
    softcap=0.0,
    num_splits=1,
    pack_gqa=None,
    deterministic=False,
    sm_margin=0,
    return_attn_probs=False,
):
    ...
</code></pre></div></div> <h2 id="math-derivation">Math Derivation</h2> <p>Below, we provide the step-by-step math derivation of the original forward / backward passes for Flex-Flash-Attention (<em>the same as Flash-Attention</em>) w/o sink tokens, and then the differences when involving the learnable attention sink mechanism, serving as the guidence for our implementations in the next section.</p> <p><b>NOTE: </b></p> <p><b>1. To simplify the derivation, we drop the <code class="language-plaintext highlighter-rouge">batch</code> dimension and only keep the <code class="language-plaintext highlighter-rouge">num_heads</code> dimension to the leftmost acting as the implicit <code class="language-plaintext highlighter-rouge">batch</code> dimension.</b></p> <p><b>2. To focus on the attention sink mechanism, we assume you’re already familiar with Flash Attention and will skip over its finer details, like the double-loop tiling strategy and the derivation of online softmax correction based on <code class="language-plaintext highlighter-rouge">log-sum-exp</code> operations.</b></p> <p><b>3. If you are new to Flash Attention or well-interested in the full original math derivation, we highly recommend this blog post: <a href="https://github.com/Strivin0311/llms-learning/blob/main/dev/modeling/lm/transformer/attn/fa2_deri.md" rel="external nofollow noopener" target="_blank">Flash Attention 2 Math Derivation</a>.</b></p> <p><b>Symbol Notation:</b></p> <table> <thead> <tr> <th>symbol</th> <th>notation</th> </tr> </thead> <tbody> <tr> <td>$\times$</td> <td>matrix multiplication</td> </tr> <tr> <td>$\cdot$</td> <td>scalar multiplication</td> </tr> <tr> <td>$\odot$</td> <td>element-wise multiplication (Hadamard product)</td> </tr> <tr> <td>$sq, sk, s\_sink$</td> <td>the sequence length of query tokens, key tokens, and attention sink tokens</td> </tr> <tr> <td>$nhq, nhk$</td> <td>the number of heads of query tokens and key tokens</td> </tr> <tr> <td>$hd$</td> <td>the head dimension of query, key and value tokens</td> </tr> <tr> <td>$X_i$</td> <td>the column vector made by the $i$-th row of matrix $X$ along the sequence dimension</td> </tr> </tbody> </table> <h3 id="ffa-forward">FFA Forward</h3> <h4 id="ffa-forward-wo-sink-tokens">FFA forward w/o sink tokens</h4> <ul> <li>step1:</li> </ul> \[\begin{aligned} &amp;S = Q \times K^{\mathrm T} \cdot scale\; + \; bias \notag \\ &amp;where\; Q \in \mathbb{R}^{nhq \times sq\times hd}, K \in \mathbb{R}^{nhk\times sk \times hd}, \notag \\ &amp; scale \in \mathbb{R}^{}, \; bias \in \mathbb{R}^{nhq\times sq\times sk}, \; S \in \mathbb{R}^{nhq\times sq\times sk} \notag \end{aligned}\] <ul> <li>step2:</li> </ul> \[\begin{aligned} &amp; softmax_{row}(X_i) = \cfrac{\mathrm{exp}(X_i - M_i)}{L_i}, \; i \in [1,sq]\notag \\ &amp; where\; M_i = \mathrm{rowmax}(X_i), \; L_i = \mathrm{rowsum}(\mathrm{exp}(X_i - M_i))\notag \end{aligned}\] \[\begin{aligned} &amp;P = \mathrm{softmax}_{row}(S) \notag \\ &amp;where\; S, P \in \mathbb{R}^{nhq\times sq\times sk} \notag \end{aligned}\] <ul> <li>step3:</li> </ul> \[\begin{aligned} &amp;O = P \times V, \;\mathrm{LSE}_i = \log(L_i) + M_i, \; i \in [1, sq]\notag \\ &amp;where\; P \in \mathbb{R}^{nhq\times sq\times sk}, \; V \in \mathbb{R}^{nhk\times sk\times hd}, \notag \\ &amp; O \in \mathbb{R}^{nhq\times sq\times hd}, \;\mathrm{LSE} \in \mathbb{R}^{nhq\times sq}\notag \end{aligned}\] <h4 id="ffa-forward-with-sink-tokens">FFA forward with sink tokens</h4> <ul> <li> <p>step1: (<em>the same</em>)</p> </li> <li> <p>step2:</p> </li> </ul> \[\begin{aligned} &amp;\tilde{P} = \mathrm{softmax}_{row}(\tilde{S}),\;\tilde{S}_i = [S_i, sink], \; i \in [1, sq] \notag \\ &amp;where\; \tilde{S},\tilde{P} \in \mathbb{R}^{nhq\times sq\times (sk+s\_sink)} ,\;sink \in \mathbb{R}^{nhq\times s\_sink}\notag \\ \end{aligned}\] \[\begin{aligned} &amp; \tilde{P}_i = [\tilde{P}^{qk}_{i}, P^{sink}_{i}],\; i \in [1, sq] \notag \\ &amp;where\; \tilde{P}^{qk} \in \mathbb{R}^{nhq\times sq\times sk} ,\notag\\ &amp;P^{sink} \in \mathbb{R}^{nhq\times sq\times s\_sink} \notag \\ \end{aligned}\] <ul> <li>step3:</li> </ul> \[\begin{aligned} &amp;\tilde{O} = \tilde{P}^{qk} \times V, \;\tilde{\mathrm{LSE}}_i = \log(\tilde{L}_i) + M_i, \; i \in [1, sq]\notag \\ &amp; \tilde{L}_i = L_i + \sum\limits_{j=1}^{s\_sink}\mathrm{exp}(sink_j - M_i), \; i \in [1, sq]\notag \\ &amp; \tilde{P}^{qk}_i = P^{qk}_i \times \cfrac{L_i}{\tilde{L}_i}, \; i \in [1, sq]\notag \\ &amp;where\; P^{qk},\tilde{P}^{qk} \in \mathbb{R}^{nhq\times sq\times sk}, \; V \in \mathbb{R}^{nhk\times sk\times hd}, \notag \\ &amp; \tilde{O} \in \mathbb{R}^{nhq\times sq\times hd}, \;\tilde{\mathrm{LSE}} \in \mathbb{R}^{nhq\times sq}\notag \end{aligned}\] <ul> <li> <b>sink correction</b>: (<em>as a post-processing of original ffa forward w/o sink tokens</em>)</li> </ul> \[\begin{aligned} &amp; \mathrm{LSE}^{sink} = \log\big( \sum\limits_{j=1}^{s\_sink}\mathrm{exp}(sink_j)\big)\notag \\ &amp; \tilde{\mathrm{LSE}}_i = \log\big(\exp(\mathrm{LSE}_i) + \exp(\mathrm{LSE}^{sink})\big), \; i \in [1, sq]\notag \\ &amp;\tilde{O} = O \cdot \exp\big(\mathrm{LSE} - \tilde{\mathrm{LSE}} \big)\notag \\ &amp;where\; sink \in \mathbb{R}^{nhq\times s\_sink},\;\mathrm{LSE}^{sink} \in \mathbb{R}^{nhq}\notag\\ &amp;\mathrm{LSE},\tilde{\mathrm{LSE}} \in \mathbb{R}^{nhq\times sq}, \;O,\tilde{O}\in \mathbb{R}^{nhq\times sq\times hd}\;\notag \end{aligned}\] <h3 id="ffa-backward">FFA Backward</h3> <h4 id="ffa-backward-wo-sink-tokens">FFA backward w/o sink tokens</h4> <ul> <li>step1: (<em>as a pre-processing</em>)</li> </ul> \[\begin{aligned} &amp;\Delta_i = P^{\mathrm T}_i \times dP_i = O^{\mathrm T}_i \times dO_i, \; i \in [1, sq] \notag\\ &amp;\Delta = \mathrm{sum}_{hd}(O \;\odot\; dO) \notag\\ &amp;where\; O,dO \in \mathbb{R}^{nhq\times sq\times hd}, \; \Delta \in \mathbb{R}^{nhq\times sq} \notag \end{aligned}\] <ul> <li>step2:(<em>recomputation</em>)</li> </ul> \[\begin{aligned} &amp;S = Q \times K^{\mathrm T} \cdot scale\; + \; bias \notag \\ &amp;P_i = \exp\big(S_i - \mathrm{LSE}_i), \; i \in [1, sq] \notag \\ &amp;where\; Q \in \mathbb{R}^{nhq \times sq\times hd}, K \in \mathbb{R}^{nhk\times sk \times hd}, \;scale \in \mathbb{R}^{}\notag \\ &amp;bias \in \mathbb{R}^{nhq\times sq\times sk}, \; S,P \in \mathbb{R}^{nhq\times sq\times sk}, \;\mathrm{LSE} \in \mathbb{R}^{nhq\times sq} \notag \end{aligned}\] <ul> <li>step3:</li> </ul> \[\begin{aligned} &amp;dV = P^{\mathrm T} \times dO \notag \\ &amp;dP = dO \times V^{\mathrm T} \notag \\ &amp;where\; P,dP \in \mathbb{R}^{nhq\times sq\times sk}\notag\\ &amp;V,dV \in \mathbb{R}^{nhk\times sk\times hd} \notag \\ &amp;dO \in \mathbb{R}^{nhq\times sq\times hd} \notag \end{aligned}\] <ul> <li>step4:</li> </ul> \[\begin{aligned} &amp;dS_i = P_i \odot (dP_i - \Delta_i), \; i \in [1, sq] \notag \\ &amp;where\; P,dP \in \mathbb{R}^{nhq\times sq\times sk}\notag\\ &amp;dS \in \mathbb{R}^{nhq\times sq\times sk},\;\Delta \in \mathbb{R}^{nhq\times sq} \notag \\ \end{aligned}\] <ul> <li>step5:</li> </ul> \[\begin{aligned} &amp;\hat{dS} = dS \cdot scale \notag \\ &amp;dQ = \hat{dS} \times K \notag \\ &amp;dK = \hat{dS}^{\mathrm T} \times Q \notag \\ &amp;where\; dS,\hat{dS},bias \in \mathbb{R}^{nhq\times sq\times sk}, \;scale \in \mathbb{R}^{}\notag \\ &amp;Q,dQ \in \mathbb{R}^{nhq\times sq\times hd}, \; K,dK \in \mathbb{R}^{nhk\times sk\times hd} \notag \end{aligned}\] <h4 id="ffa-backward-with-sink-tokens">FFA backward with sink tokens</h4> <ul> <li>step1: (<em>as a pre-processing as well</em>)</li> </ul> \[\begin{aligned} &amp;\tilde{\Delta}_i = \tilde{P}^{\mathrm T}_i \times dP_i = [\tilde{P}^{qk}_i, P^{sink}_i]^{\mathrm T} \times [dP^{qk}_i, \underbrace{dP^{sink}_i}_{zeros}] \notag\\ &amp;= {\tilde{P}^{qk}_i}^{\mathrm T} \times dP^{qk}_i \;+\; {P^{sink}_i}^{\mathrm T} \times \underbrace{dP^{sink}_i}_{zeros}\notag\\ &amp;= {\tilde{P}^{qk}_i}^{\mathrm T} \times dP^{qk}_i = \tilde{O}^{\mathrm T}_i \times dO_i, \; i \in [1, sq] \notag\\ &amp;\tilde{\Delta} = \mathrm{sum}_{hd}(\tilde{O} \;\odot\; dO) \notag\\ &amp;where\; \tilde{O},dO \in \mathbb{R}^{nhq\times sq\times hd}, \; \tilde{\Delta} \in \mathbb{R}^{nhq\times sq} \notag \\ &amp;\tilde{P},dP \in \mathbb{R}^{nhq\times sq\times (sk+s\_sink)} \notag \end{aligned}\] <ul> <li>step2:(<em>recomputation</em>)</li> </ul> \[\begin{aligned} &amp;S = Q \times K^{\mathrm T} \cdot scale\; + \; bias \notag \\ &amp;\tilde{S}_i = [S_i, sink], \; i \in [1, sq] \notag \\ &amp;\tilde{P}_i = \exp\big(\tilde{S}_i - \tilde{\mathrm{LSE}}_i), \; i \in [1, sq] \notag \\ &amp; \tilde{P}_i = [\tilde{P}^{qk}_{i}, P^{sink}_{i}],\; i \in [1, sq] \notag \\ &amp;where\; Q \in \mathbb{R}^{nhq \times sq\times hd}, K \in \mathbb{R}^{nhk\times sk \times hd}, \;scale \in \mathbb{R}^{}\notag \\ &amp;bias \in \mathbb{R}^{nhq\times sq\times sk}, \; \tilde{S},\tilde{P} \in \mathbb{R}^{nhq\times sq\times (sk+s\_sink)}, \;\tilde{\mathrm{LSE}} \in \mathbb{R}^{nhq\times sq} \notag \\ &amp;\tilde{P}^{qk} \in \mathbb{R}^{nhq\times sq\times sk} ,\; P^{sink} \in \mathbb{R}^{nhq\times sq\times s\_sink} \notag \\ \end{aligned}\] <ul> <li>step3:</li> </ul> \[\begin{aligned} &amp;dV = \tilde{P}^{\mathrm T} \times dO \notag \\ &amp;dP = dO \times V^{\mathrm T} \notag \\ &amp;where\; \tilde{P},dP \in \mathbb{R}^{nhq\times sq\times sk}\notag\\ &amp;V,dV \in \mathbb{R}^{nhk\times sk\times hd} \notag \\ &amp;dO \in \mathbb{R}^{nhq\times sq\times hd} \notag \end{aligned}\] <ul> <li>step4:</li> </ul> \[\begin{aligned} &amp;\tilde{dS}_i = [dS_{i}, dsink_{i}] = \tilde{P}_i \odot (dP_i - \tilde{\Delta}_i) \notag\\ &amp;= [\tilde{P}^{qk}_{i}, P^{sink}_{i}] \odot ([dP^{qk}_i, \underbrace{dP^{sink}_i}_{zeros}] - \tilde{\Delta}_i)\notag\\ &amp;= [\tilde{P}^{qk}_{i} \odot (dP^{qk}_i- \tilde{\Delta}_i), P^{sink}_{i}\odot (\underbrace{dP^{sink}_i}_{zeros}- \tilde{\Delta}_i)] \notag \\ &amp;= [\underbrace{\tilde{P}^{qk}_{i} \odot (dP^{qk}_i- \tilde{\Delta}_i)}_{dS_{i}}, \underbrace{P^{sink}_{i}\cdot - \tilde{\Delta}_i]}_{dsink_{i}}, \; i \in [1, sq] \notag \\ &amp;dsink = \sum\limits_{i=1}^{sq} dsink_i = \sum\limits_{i=1}^{sq} \big(P^{sink}_{i}\cdot - \tilde{\Delta}_i\big) = {P^{sink}}^{\mathrm T} \times -\tilde{\Delta}\notag\\ &amp;where\; \tilde{P},dP,\tilde{dS} \in \mathbb{R}^{nhq\times sq\times (sk+s\_sink)}\notag\\ &amp;dS \in \mathbb{R}^{nhq\times sq\times sk},\;\tilde{\Delta} \in \mathbb{R}^{nhq\times sq}, \; dsink \in \mathbb{R}^{nhq\times s\_sink} \notag \\ &amp; P^{sink} \in \mathbb{R}^{nhq\times sq\times s\_sink}\notag \end{aligned}\] <ul> <li> <p>step5: (<em>the same</em>)</p> </li> <li> <p><b>dsink computation</b>: (<em>as another pre-processing of original ffa backward w/o sink tokens</em>)</p> </li> </ul> \[\begin{aligned} &amp;dsink = {P^{sink}}^{\mathrm T} \times -\tilde{\Delta} = \sum\limits_{i=1}^{sq} \big(P^{sink}_{i}\cdot - \tilde{\Delta}_i\big) \notag\\ &amp;= -\sum\limits_{i=1}^{sq} \big(\exp(sink - \tilde{\mathrm{LSE}}_i)\cdot \tilde{\Delta}_i\big)\notag\\ &amp;where\; sink,dsink \in \mathbb{R}^{nhq\times s\_sink},\;\tilde{\mathrm{LSE}}, \tilde{\Delta} \in \mathbb{R}^{nhq\times sq}\notag \end{aligned}\] <h2 id="implementations">Implementations</h2> <p>Based on the math derivation in the previous section, folding a learnable attention sink into the attention implementations in the Flash Attention style boils down to just two edits:</p> <ul> <li>For forward pass, we have nothing to change about the original implementation, but should apply an additional post-processing to correct the returned <code class="language-plaintext highlighter-rouge">out</code> and <code class="language-plaintext highlighter-rouge">lse</code> with <code class="language-plaintext highlighter-rouge">sink</code> tokens (<em>see the <b>sink correction</b> of the <a href="#ffa-forward-with-sink-tokens">FFA forward with sink tokens</a></em>).</li> <li>For backward pass, we have nothing to change about the original implementation, but should apply an additional pre-processing to compute the <code class="language-plaintext highlighter-rouge">dsink</code>, i.e. the gradient of <code class="language-plaintext highlighter-rouge">sink</code> (<em>see the <b>dsink computation</b> of the <a href="#ffa-backward-with-sink-tokens">FFA backward with sink tokens</a></em>).</li> </ul> <p>Therefore, we share the following code snippets to present our implementations of the learnable attention sink mechanism: a naive PyTorch reference, Flex-Flash-Attention (<em>both internal and external to the kernels, which fit Flash Attention as well</em>), and the distributed implementation of MagiAttention.</p> <h3 id="torch-reference">Torch Reference</h3> <ul> <li>reference implementation w/o sink tokens:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># apply `S = Q x K.T * scale + bias`
# where S.shape = [nhq, sq, sk]
</span><span class="n">s</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">softmax_scale</span> <span class="o">+</span> <span class="n">bias</span>

<span class="c1"># apply row-wise lse `LSE = logsumexp(S, dim=-1)`
# where LSE.shape = [nhq, sq, 1]
</span><span class="n">lse</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="nf">logsumexp</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># apply row-wise softmax `P = softmax(S, dim=-1)`
# where P.shape = [nhq, sq, sk]
</span><span class="n">p</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">s</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># apply `O = P x V`
# where O.shape = [nhq, sq, d]
</span><span class="n">out</span> <span class="o">=</span> <span class="n">p</span> <span class="o">@</span> <span class="n">v</span>

<span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">lse</span>
</code></pre></div></div> <ul> <li>reference implementation difference with sink tokens:</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code># apply `S = Q x K.T * scale + bias`
# where S.shape = [nhq, sq, sk]
<span class="p">s = q @ k.T * softmax_scale + bias
</span><span class="err">
</span><span class="gi">+ # apply `S = S.concat(sink, dim=-1)`
+ # where S.shape = [nhq, sq, sk + s_sink]
+ s = torch.concat([s, sink], dim=-1)
</span><span class="err">
</span># apply row-wise lse `LSE = logsumexp(S, dim=-1)`
# where LSE.shape = [nhq, sq, 1]
<span class="p">lse = s.logsumexp(dim=-1, keepdim=True)
</span><span class="err">
</span># apply row-wise softmax `P = softmax(S, dim=-1)`
<span class="gd">- # where P.shape = [nhq, sq, sk]
</span><span class="gi">+ # where P.shape = [nhq, sq, sk + s_sink]
</span><span class="p">p = softmax(s).to(q.dtype)
</span><span class="err">
</span><span class="gi">+ # apply `P = P.drop(sink, dim=-1)`
+ # where P.shape = [nhq, sq, sk]
+ p = p[..., : -sink.size(dim=-1)]
</span><span class="err">
</span># apply `O = P x V`
# where O.shape = [nhq, sq, d]
<span class="p">out = p @ v
</span><span class="err">
</span><span class="p">return out, lse
</span></code></pre></div></div> <h3 id="ffa-impl">FFA Impl</h3> <h4 id="ffa-forward-impl">FFA Forward Impl</h4> <h5 id="external-impl">External Impl</h5> <ul> <li>Use <b>sink correction</b> to correct <code class="language-plaintext highlighter-rouge">out</code>, <code class="language-plaintext highlighter-rouge">lse</code> after the ffa forward kernel returns, as an external post-processing kernel (<em>which is the way we extend the Flash Attention 2/3 forward with sink tokens, and see the <a href="https://github.com/SandAI-org/MagiAttention/blob/main/extensions/fa3_interface_with_sink.py" rel="external nofollow noopener" target="_blank">source code</a> for more detals</em>):</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># given sink with shape: [s_sink, nhq]
# calculate and repeat to lse_sink with shape: [sq, nhq]
</span><span class="n">lse_sink</span> <span class="o">=</span> <span class="n">sink</span><span class="p">.</span><span class="nf">logsumexp</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="n">sq</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># given ffa returned lse with shape: [sq, nhq]
# correct lse with lse_sink
</span><span class="n">corrected_lse</span> <span class="o">=</span> <span class="nf">log</span><span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="n">lse</span><span class="p">)</span> <span class="o">+</span> <span class="nf">exp</span><span class="p">(</span><span class="n">lse_sink</span><span class="p">))</span>

<span class="c1"># given ffa returned out with shape: [sq, nhq, hd]
# correct out with corrected_lse and original lse
</span><span class="n">out</span> <span class="o">*=</span> <span class="nf">exp</span><span class="p">(</span><span class="n">lse</span> <span class="o">-</span> <span class="n">corrected_lse</span><span class="p">)</span>

<span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">lse</span>
</code></pre></div></div> <h5 id="internal-impl">Internal Impl</h5> <ul> <li> <p>Since FFA forward already has a post-processing kernel <code class="language-plaintext highlighter-rouge">FlashAttnFwdPostprocess</code> to zero-fill up the never-stored rows of <code class="language-plaintext highlighter-rouge">O</code>, indicated by “whether the corr. row of <code class="language-plaintext highlighter-rouge">lse</code> is still <code class="language-plaintext highlighter-rouge">-inf</code>”, …</p> </li> <li> <p>Then we can fuse the <b>sink correction</b> process into the <code class="language-plaintext highlighter-rouge">FlashAttnFwdPostprocess</code> kernel as follows (<em>see the <a href="https://github.com/SandAI-org/MagiAttention/blob/main/magi_attention/csrc/flexible_flash_attention/flash_fwd_postprocess_kernel.h" rel="external nofollow noopener" target="_blank">source code</a> for more details</em>):</p> <ul> <li>As for lse correction: <ul> <li>If the current row of <code class="language-plaintext highlighter-rouge">lse</code> is not <code class="language-plaintext highlighter-rouge">-inf</code>, then we update this row of <code class="language-plaintext highlighter-rouge">lse</code> with <code class="language-plaintext highlighter-rouge">lse_sink</code>.</li> <li>Otherwise, the <code class="language-plaintext highlighter-rouge">lse</code> should also be filled up with <code class="language-plaintext highlighter-rouge">lse_sink</code>, instead of <code class="language-plaintext highlighter-rouge">-inf</code>.</li> </ul> </li> <li>As for out correction: <ul> <li>If the current row of <code class="language-plaintext highlighter-rouge">lse</code> is not <code class="language-plaintext highlighter-rouge">-inf</code>, then load the corr. row of <code class="language-plaintext highlighter-rouge">O</code>, rescale it and write it back.</li> <li>Otherwise, the corr. row of <code class="language-plaintext highlighter-rouge">O</code> still needs to be filled up with <code class="language-plaintext highlighter-rouge">0</code>, so the same as before.</li> </ul> </li> </ul> </li> </ul> <h4 id="ffa-backward-impl">FFA Backward Impl</h4> <h5 id="external-impl-1">External Impl</h5> <ul> <li>Use <b>dsink computation</b> to compute dsink before the ffa backward kernel launchs, as an external pre-processing kernel (<em>which is the way we extend the Flash Attention 2/3 backward with sink tokens, and see the <a href="https://github.com/SandAI-org/MagiAttention/blob/main/extensions/fa3_interface_with_sink.py" rel="external nofollow noopener" target="_blank">source code</a> for more detals</em>):</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># calculate delta = (o * do).sum(dim=-1)
# where o.shape = [sq, nhq, d]
#       do.shape = [sq, nhq, d]
#       delta.shape = [nhq, sq, 1]
</span><span class="n">delta</span> <span class="o">=</span> <span class="nf">reduce</span><span class="p">((</span><span class="n">o</span> <span class="o">*</span> <span class="n">do</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">lse</span><span class="p">.</span><span class="n">dtype</span><span class="p">),</span> <span class="sh">"</span><span class="s">sq hq d -&gt; hq sq 1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># calculate p_sink = exp(sink - lse)
# where sink.shape = [nhq, sq, s_sink]
#       lse.shape = [nhq, sq, 1]
#       p_sink.shape = [nhq, sq, s_sink]
</span><span class="n">p_sink</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">sink</span> <span class="o">-</span> <span class="n">lse</span><span class="p">)</span>

<span class="c1"># calculate dsink = p_sink.T x -delta
# where p_sink.shape = [nhq, sq, s_sink]
#       delta.shape = [nhq, sq, 1]
#       dsink.shape = [s_sink, nhq]
</span><span class="n">dsink</span> <span class="o">=</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">p_sink</span> <span class="o">*</span> <span class="o">-</span><span class="n">delta</span><span class="p">,</span> <span class="sh">"</span><span class="s">nhq sq s_sink -&gt; s_sink nhq</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">)</span>

<span class="k">return</span> <span class="n">dsink</span>
</code></pre></div></div> <h5 id="internal-impl-1">Internal Impl</h5> <ul> <li> <p>Since FFA backward already has a pre-processing kernel <code class="language-plaintext highlighter-rouge">FlashAttnBwdPreprocess</code> to compute $\Delta$ (<em>in FA / FFA, we name it <code class="language-plaintext highlighter-rouge">dPsum</code></em>), w.r.t. the step1 in the <a href="#ffa-backward-wo-sink-tokens">FFA backward w/o sink tokens</a>, …</p> </li> <li> <p>The we can fuse the <b>dsink computation</b> process into the <code class="language-plaintext highlighter-rouge">FlashAttnBwdPreprocess</code> kernel as follows (<em>see the <a href="https://github.com/SandAI-org/MagiAttention/blob/main/magi_attention/csrc/flexible_flash_attention/flash_bwd_preprocess_kernel.h" rel="external nofollow noopener" target="_blank">source code</a> for more details</em>):</p> <ul> <li> <p>As for <code class="language-plaintext highlighter-rouge">lse</code>, the same as before, each thread in one block loads one unique row of <code class="language-plaintext highlighter-rouge">lse</code>.</p> </li> <li> <p>As for <code class="language-plaintext highlighter-rouge">p_sink</code>, the first <code class="language-plaintext highlighter-rouge">seqlen_sink</code> of threads in one block load the <code class="language-plaintext highlighter-rouge">sink</code> to shared memory, and each thread computes <code class="language-plaintext highlighter-rouge">p_sink = exp(sink - lse)</code> with its own unique row of <code class="language-plaintext highlighter-rouge">lse</code>, storing to shared memory as well.</p> </li> <li> <p>As for <code class="language-plaintext highlighter-rouge">dPsum</code>, the same as before, each block loads a unique <code class="language-plaintext highlighter-rouge">kBlockM</code> rows of <code class="language-plaintext highlighter-rouge">O</code> and <code class="language-plaintext highlighter-rouge">dO</code>, applies <code class="language-plaintext highlighter-rouge">O * dO</code>, reduces across the head dimension to get the local block of <code class="language-plaintext highlighter-rouge">dPsum</code> in register files, and stores it to global memory.</p> </li> <li> <p>As for <code class="language-plaintext highlighter-rouge">d_sink</code>, since it requires to be reduced across the whole <code class="language-plaintext highlighter-rouge">seqlen_q</code> dimension, the following steps are performed:</p> <ul> <li>step1: each thread loads a unique row of <code class="language-plaintext highlighter-rouge">dPsum</code> from register files and the corr. row of <code class="language-plaintext highlighter-rouge">p_sink</code> from shared memory, and computes thread-partial <code class="language-plaintext highlighter-rouge">dsink = p_sink * -dPsum</code> for this row, and stores to shared memory first (<em>since <code class="language-plaintext highlighter-rouge">p_sink</code> is not used afterwards, we can reuse its shared memory buffer to store <code class="language-plaintext highlighter-rouge">dsink</code></em>).</li> <li>step2: each block loads all the thread-partial <code class="language-plaintext highlighter-rouge">dsink</code> from shared memory, applies a <code class="language-plaintext highlighter-rouge">block-reduction</code> to get the block-reduced <code class="language-plaintext highlighter-rouge">dsink</code> for these <code class="language-plaintext highlighter-rouge">kBlockM</code> rows, and stores it to a temporary buffer in global memory.</li> <li>step3: after a device-level memory fence, the last block who stores its block-reduced <code class="language-plaintext highlighter-rouge">dsink</code> loads all the block-reduced <code class="language-plaintext highlighter-rouge">dsink</code> back from the temporary buffer, applies another <code class="language-plaintext highlighter-rouge">block-reduction</code> to get the reduced <code class="language-plaintext highlighter-rouge">dsink</code> across the whole <code class="language-plaintext highlighter-rouge">seqlen_q</code> dimension, and finally stores it to global memory.</li> </ul> </li> </ul> </li> </ul> <h3 id="magiattn-impl">MagiAttn Impl</h3> <h4 id="magiattn-forward">MagiAttn Forward</h4> <ul> <li>Since <code class="language-plaintext highlighter-rouge">sink</code> is replicated across cp ranks, we can easily apply attention sink by just passing <code class="language-plaintext highlighter-rouge">sink</code> into <code class="language-plaintext highlighter-rouge">_flex_flash_attn_forward</code>.</li> <li>However, the attention sink is supposed to be applied <u>once and only once</u> for the same query token, thus we can apply it at the host stage, i.e. each cp rank only applies to their own local <code class="language-plaintext highlighter-rouge">q</code>.</li> <li>Then, If the host stage is not skipped, just apply attention sink by passing <code class="language-plaintext highlighter-rouge">sink</code> into <code class="language-plaintext highlighter-rouge">_flex_flash_attn_forward</code>:</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">partial_out, partial_lse = _flex_flash_attn_forward(
</span>    q=q,
    k=k,
    v=v,
<span class="gi">+   # NOTE: sink token needs to be applied only once
+   # thus we only apply it at the host stage if not skipped
+   sink=sink if is_host_stage else None,
</span>    out=out_acc,
    lse=lse_acc,
    **attn_arg.to_ffa_args(is_bwd=False),
    ...
)
</code></pre></div></div> <ul> <li>Otherwise, we should zero-initialize <code class="language-plaintext highlighter-rouge">local_out</code> as before, but initialize <code class="language-plaintext highlighter-rouge">local_lse</code> with <code class="language-plaintext highlighter-rouge">lse_sink</code>, instead of <code class="language-plaintext highlighter-rouge">-inf</code> </li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">out = torch.zeros_like(
</span>    q,
    dtype=torch.float32,
    device=q.device,
)
<span class="err">
</span><span class="gi">+ if sink is not None:
+   # in skipped host stage if sink is given,
+   # we directly use lse_sink to initialize lse
+   lse = calc_lse_sink(
+       sink=sink,
+       seqlen_lse=q.size(0),
+   )
+ else:
</span>    lse = torch.full(
        (q.size(0), q.size(1)),
        fill_value=float("-inf"),
        dtype=float32,
        device=q.device,
    )
    
<span class="p">return out, lse
</span></code></pre></div></div> <h4 id="magiattn-backward">MagiAttn Backward</h4> <ul> <li>The same to the forward, to form a complete, non-overlapping breakdown of <code class="language-plaintext highlighter-rouge">dsink</code> computation, we can compute partial <code class="language-plaintext highlighter-rouge">dsink</code> by just passing <code class="language-plaintext highlighter-rouge">sink</code> into <code class="language-plaintext highlighter-rouge">_flex_flash_attn_backward</code> only at the host stage, if not skipped.</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(
    partial_dq,
    partial_dk,
    partial_dv,
<span class="gi">+   partial_dsink,
</span>) = _flex_flash_attn_backward(
    dout=do,
    q=q,
    k=k,
    v=v,
<span class="gi">+   # NOTE: dsink should be computed only once
+   # thus we only compute it at the host stage if not skipped
+   sink=sink if is_host_stage else None,
</span>    out=o,
    lse=lse,
    dq=dq_acc,
    dk=partial_dk,
    dv=partial_dv,
<span class="gi">+   dsink=None,  # let kernel initialize dsink if required
</span>    **attn_arg.to_ffa_args(is_bwd=True),
    ...
)
</code></pre></div></div> <ul> <li>And according to the formula of <b>dsink computation</b>, <code class="language-plaintext highlighter-rouge">dsink</code> is required to be sum-reduced along the <code class="language-plaintext highlighter-rouge">seqlen_q</code> dim, therefore, to get the reduced <code class="language-plaintext highlighter-rouge">dsink</code> for each cp rank, we have to additionally launch an all-reduce communication with <code class="language-plaintext highlighter-rouge">ReduceOp.Sum</code>, and wait it to complete before returning from the backward.</li> <li>However, the tricky thing is that during the acutal training scenario, the learnable <code class="language-plaintext highlighter-rouge">sink</code> tensor will be considered as a regular parameter in the model similar to <code class="language-plaintext highlighter-rouge">bias</code> in <code class="language-plaintext highlighter-rouge">nn.Linear</code> layer. So under some popular training frameworks, such as <code class="language-plaintext highlighter-rouge">Megatron-LM</code>, <code class="language-plaintext highlighter-rouge">FSDP</code>, the sum-reduction across cp ranks of the partial gradients of <code class="language-plaintext highlighter-rouge">sink</code> might be automatically applied within the whole <code class="language-plaintext highlighter-rouge">dp x cp</code> mesh.</li> <li>To avoid repeated reduction, we provide the environment variable <code class="language-plaintext highlighter-rouge">MAGI_ATTENTION_DSINK_ALL_REDUCE_OP</code> to let the user specify the all-reduce op for <code class="language-plaintext highlighter-rouge">dsink</code> within MagiAttention (<em>see the <a href="https://sandai-org.github.io/MagiAttention/docs/main/env_variables.html#for-correctness" rel="external nofollow noopener" target="_blank">docs</a> for more details</em>). Defaults to <code class="language-plaintext highlighter-rouge">none</code> to <b>NOT</b> apply any reduction to <code class="language-plaintext highlighter-rouge">dsink</code> and let the framework handle it. Other options include <code class="language-plaintext highlighter-rouge">sum</code> and <code class="language-plaintext highlighter-rouge">avg</code> if needed.</li> </ul> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gi">+ # after the host stage when the partial dsink is ready
+ work = dist.all_reduce(
+    dsink,
+    op=dsink_reduce_op, # specified by `MAGI_ATTENTION_DSINK_ALL_REDUCE_OP`
+    group=self.cp_group_gc,
+    async_op=True,
+ )
</span><span class="err">
</span>...
<span class="err">
</span><span class="gi">+ # before returning from the backward
+ work.wait()
</span><span class="err">
</span>...
<span class="err">
</span><span class="gd">- return dq, dk, dv, ...
</span><span class="gi">+ return dq, dk, dv, dsink, ...
</span></code></pre></div></div> <h2 id="citation">Citation</h2> <p>If you find MagiAttention useful in your research, please cite:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">magiattention2025</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{MagiAttention: A Distributed Attention Towards Linear Scalability for Ultra-Long Context, Heterogeneous Mask Training}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zewei, Tao and Yunpeng, Huang}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
  <span class="na">howpublished</span><span class="p">=</span><span class="s">{\url{https://github.com/SandAI-org/MagiAttention/}}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/MagiAttention/blog/assets/bibliography/ffa_with_sink.bib"></d-bibliography> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 SandAI . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/MagiAttention/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/MagiAttention/blog/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/MagiAttention/blog/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/MagiAttention/blog/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/MagiAttention/blog/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/MagiAttention/blog/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/MagiAttention/blog/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/MagiAttention/blog/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/MagiAttention/blog/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/MagiAttention/blog/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/MagiAttention/blog/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/MagiAttention/blog/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/MagiAttention/blog/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/MagiAttention/blog/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/MagiAttention/blog/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/MagiAttention/blog/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/MagiAttention/blog/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>