---
blogpost: true
date: Feb 14, 2026
author: Yunpeng Huang
location: China
category: MagiAttention
tags: Flex-Flash-Attention, Distributed Attention, Context Parallelism
language: English
---

# Distributed-Native FFA

:::{todo}
This is a placeholder for the upcoming blog post, which will be released in the near future. Stay tuned!
:::

## Citation


If you find MagiAttention useful in your research, please cite:

```bibtex
@misc{magiattention2025,
  title={MagiAttention: A Distributed Attention Towards Linear Scalability for Ultra-Long Context, Heterogeneous Mask Training},
  author={Zewei, Tao and Yunpeng, Huang},
  year={2025},
  howpublished={\url{https://github.com/SandAI-org/MagiAttention/}},
}
```

## References

```{bibliography} refs/dist_native.bib
```
