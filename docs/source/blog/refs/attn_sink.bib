@misc{xiao2024efficientstreaminglanguagemodels,
      title={Efficient Streaming Language Models with Attention Sinks}, 
      author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
      year={2024},
      eprint={2309.17453},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.17453}, 
}

@misc{gu2025attentionsinkemergeslanguage,
      title={When Attention Sink Emerges in Language Models: An Empirical View}, 
      author={Xiangming Gu and Tianyu Pang and Chao Du and Qian Liu and Fengzhuo Zhang and Cunxiao Du and Ye Wang and Min Lin},
      year={2025},
      eprint={2410.10781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.10781}, 
}

@misc{miller2025attentionmisc,
  author       = {Evan Miller},
  title        = {Attention is Off by One},
  howpublished = {\url{https://www.evanmiller.org/attention-is-off-by-one.html}},
  year         = {2024},
  month        = {july},
}

@misc{openaiGPT-OSScode-misc,
  author       = {{OpenAI}},
  title        = {GPT-{OSS} v0.0.8 model.py},
  howpublished = {\url{https://github.com/openai/gpt-oss/blob/v0.0.8/gpt_oss/torch/model.py#L169}},
  year         = {2025},
}

@article{dao2022flashattention_attn_sink,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{dao2023flashattention_attn_sink,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@misc{shah2024flashattention3fastaccurateattention_attn_sink,
      title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision}, 
      author={Jay Shah and Ganesh Bikshandi and Ying Zhang and Vijay Thakkar and Pradeep Ramani and Tri Dao},
      year={2024},
      eprint={2407.08608},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.08608}, 
}

@misc{magiattention2025_attn_sink,
  title={MagiAttention: A Distributed Attention Towards Linear Scalability for Ultra-Long Context, Heterogeneous Mask Training},
  author={Zewei, Tao and Yunpeng, Huang},
  year={2025},
  howpublished={\url{https://github.com/SandAI-org/MagiAttention/}},
}

@misc{daoFlashAttnInterfaceMisc,
  author       = {{Dao-AILab}},
  title        = {flash_attn_interface.py (Flash-Attention v2.8.3)},
  howpublished = {\url{https://github.com/Dao-AILab/flash-attention/blob/v2.8.3/flash_attn/flash_attn_interface.py}},
  year         = {2025},
}

@misc{daoFlashAttnInterfaceHopperMisc,
  author       = {{Dao-AILab}},
  title        = {hopper/flash_attn_interface.py (Flash-Attention v2.8.3)},
  howpublished = {\url{https://github.com/Dao-AILab/flash-attention/blob/v2.8.3/hopper/flash_attn_interface.py}},
  year         = {2025},
}

@misc{kang2025toldvisualattentionsink,
      title={See What You Are Told: Visual Attention Sink in Large Multimodal Models}, 
      author={Seil Kang and Jinyeong Kim and Junhyeok Kim and Seong Jae Hwang},
      year={2025},
      eprint={2503.03321},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.03321}, 
}
