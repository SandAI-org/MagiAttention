@misc{magiattention2025_blackwell_ffa_fa4,
  title={MagiAttention: A Distributed Attention Towards Linear Scalability for Ultra-Long Context, Heterogeneous Mask Training},
  author={Zewei, Tao and Yunpeng, Huang},
  year={2025},
  howpublished={\url{https://github.com/SandAI-org/MagiAttention/}},
}

@misc{dao2025flashattention_cute_blackwell_ffa_fa4,
  author       = {Dao, Tri and Driss, Guessous and Henry, Tsang},
  title        = {FlashAttention CUTE Module [Software Documentation]},
  howpublished = {GitHub Repository README},
  year         = {2025},
  publisher    = {Dao-AILab},
  url          = {https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/cute/README.md},
}
