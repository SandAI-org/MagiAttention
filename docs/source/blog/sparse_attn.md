---
blogpost: true
date: Jan 25, 2026
author: Zewei Tao, Hanwen Sun, Bowen Zeng, Jin Li, Yunpeng Huang
location: China
category: MagiAttention
tags: Sparse Attention, NSA, DSA, Flex-Flash-Attention, Flash-Attention
language: English
---

# Optimize Sparse Attention in FFA

:::{todo}
This is a placeholder for the upcoming blog post, which will be released in the near future. Stay tuned!
:::

## Citation


If you find MagiAttention useful in your research, please cite:

```bibtex
@misc{magiattention2025,
  title={MagiAttention: A Distributed Attention Towards Linear Scalability for Ultra-Long Context, Heterogeneous Mask Training},
  author={Zewei, Tao and Yunpeng, Huang},
  year={2025},
  howpublished={\url{https://github.com/SandAI-org/MagiAttention/}},
}
```

## References

```{bibliography} refs/sparse_attn.bib
```
