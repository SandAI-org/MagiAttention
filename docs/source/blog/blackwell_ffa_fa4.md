---
blogpost: true
date: Feb 7, 2026
author: Jerry Chen, Yujia Liu, Yufeng Yang, Yunpeng Huang, Zewei Tao, Qiangang Wang, Kunlun Li
location: China
category: MagiAttention
tags: Blackwell, Flex-Flash-Attention, Flash-Attention, HSTU Function Representation
language: English
---

# Support Blackwell with FFA_FA4 Backend

:::{todo}
The upcoming blog post will be released in the near future. Stay tuned!
:::

## Citation


If you find MagiAttention useful in your research, please cite:

```bibtex
@misc{magiattention2025,
  title={MagiAttention: A Distributed Attention Towards Linear Scalability for Ultra-Long Context, Heterogeneous Mask Training},
  author={Zewei, Tao and Yunpeng, Huang},
  year={2025},
  howpublished={\url{https://github.com/SandAI-org/MagiAttention/}},
}
```

## References

```{bibliography} refs/blackwell_ffa_fa4.bib
```
