---
blogpost: true
date: Feb 4, 2026
author: Jin Li
location: China
category: MagiAttention
tags: Muon, QK-Clip, Flex-Flash-Attention, Flash-Attention
language: English
---

# Suppport FFA with Muon QK-Clip Max Logits


## Citation


If you find MagiAttention useful in your research, please cite:

```bibtex
@misc{magiattention2025,
  title={MagiAttention: A Distributed Attention Towards Linear Scalability for Ultra-Long Context, Heterogeneous Mask Training},
  author={Zewei, Tao and Yunpeng, Huang},
  year={2025},
  howpublished={\url{https://github.com/SandAI-org/MagiAttention/}},
}
```

## References

```{bibliography} refs/muon_qk_clip_max_logits.bib
```