
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Long-Context Attention Benchmark &#8212; MagiAttention main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=3ee1c6c6" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=53070b4a" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=fd10adb8"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'blog/cp_benchmark';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://raw.githubusercontent.com/SandAI-org/MagiAttention/refs/heads/gh-pages/docs/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            true;
        </script>
    <link rel="canonical" href="https://sandai-org.github.io/MagiAttention/docs/blog/cp_benchmark.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Support Native Group Collective Based on DeepEP" href="native_grpcoll.html" />
    <link rel="prev" title="MagiAttention" href="magi_attn.html" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />   
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-black.png" class="logo__image only-light" alt=""/>
    <img src="../_static/logo-gold.png" class="logo__image only-dark pst-js-only" alt=""/>
  
  
    <p class="title logo__title">MagiAttention</p>
  
</a></div>
    
      <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide/toc.html">
    User Guide
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="toc.html">
    Blogs
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/SandAI-org/MagiAttention" title="Github" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Github</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://sandai-org.github.io/MagiAttention/blog/" title="Blog" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fas fa-book-bookmark fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Blog</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide/toc.html">
    User Guide
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="toc.html">
    Blogs
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/SandAI-org/MagiAttention" title="Github" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Github</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://sandai-org.github.io/MagiAttention/blog/" title="Blog" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fas fa-book-bookmark fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Blog</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__postcard">
   
  <h2>
     
    <i class="fa fa-calendar"></i>
    
    <span>19 October 2025</span>
    
  </h2>
  <ul>
    <div class="ablog-sidebar-item ablog__postcard2">
   
  <li id="ablog-sidebar-item author ablog__author">
    <span>
      
      <i class="fa-fw fa fa-user"></i>
      
    </span>
     
    <a href="author/tao-bu.html">Tao Bu</a>
     ,    
    <a href="author/qiangang-wang.html">Qiangang Wang</a>
     ,    
    <a href="author/bowen-zeng.html">Bowen Zeng</a>
     ,    
    <a href="author/hanwen-sun.html">Hanwen Sun</a>
     ,    
    <a href="author/yunpeng-huang.html">Yunpeng Huang</a>
     ,    
    <a href="author/zewei-tao.html">Zewei Tao</a>
      
  </li>
   
  <li id="ablog-sidebar-item location ablog__location">
    <span>
      
      <i class="fa-fw fa fa-location-arrow"></i>
      
    </span>
     
    <a href="location/china.html">China</a>
      
  </li>
   
  <li id="ablog-sidebar-item language ablog__language">
    <span>
      
      <i class="fa-fw fa fa-language"></i>
      
    </span>
     
    <a href="language/english.html">English</a>
      
  </li>
   
  <li id="ablog-sidebar-item category ablog__category">
    <span>
      
      <i class="fa-fw fa fa-folder-open"></i>
      
    </span>
     
    <a href="category/magiattention.html">MagiAttention</a>
      
  </li>
   
  <li id="ablog-sidebar-item tags ablog__tags">
    <span>
       
      <i class="fa-fw fa fa-tags"></i>
       
    </span>
     
    <a href="tag/benchmark.html">Benchmark</a>
        
    <a href="tag/blackwell.html">Blackwell</a>
        
    <a href="tag/flex-flash-attention.html">Flex-Flash-Attention</a>
        
    <a href="tag/distributed-attention.html">Distributed Attention</a>
        
    <a href="tag/context-parallelism.html">Context Parallelism</a>
      
  </li>
   
</div>
  </ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__recentposts">
  <h3>
    <a href="../blog.html">Recent Posts</a>
  </h3>
  <ul>
     
    <li>
      <a href="kernel_overlap.html">
        15 February - How to Ensure Kernels Actually Overlapped
      </a>
    </li>
    
    <li>
      <a href="dist_native.html">
        14 February - Distributed-Native FFA
      </a>
    </li>
    
    <li>
      <a href="attn_engine.html">
        08 February - Attention Engine for Inference
      </a>
    </li>
    
    <li>
      <a href="blackwell_ffa_fa4.html">
        07 February - Support Blackwell with FFA_FA4 Backend
      </a>
    </li>
    
    <li>
      <a href="muon_qk_clip.html">
        04 February - Support Muon QK-Clip
      </a>
    </li>
    
  </ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__tagcloud">
  <link
    rel="stylesheet"
    href="../_static/ablog/tagcloud.css"
    type="text/css"
  />
  <h3><a href="tag.html">Tags</a></h3>
  <ul class="ablog-cloud">
     
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/af-disaggregation.html">AF Disaggregation</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/attention-sink.html">Attention Sink</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/attention-slice-representation.html">Attention Slice Representation</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/benchmark.html">Benchmark</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/blackwell.html">Blackwell</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/computation-load-balance.html">Computation Load-Balance</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/computation-communication-overlap.html">Computation-Communication Overlap</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-4">
      <a href="tag/context-parallelism.html">Context Parallelism</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/dsa.html">DSA</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/deepep.html">DeepEP</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-4">
      <a href="tag/distributed-attention.html">Distributed Attention</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/dynamic-load-balance.html">Dynamic Load Balance</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-3">
      <a href="tag/flash-attention.html">Flash-Attention</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-5">
      <a href="tag/flex-flash-attention.html">Flex-Flash-Attention</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/group-collective.html">Group Collective</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/hstu-function-representation.html">HSTU Function Representation</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/hybrid-attention.html">Hybrid Attention</a>
    </li>
        
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/multi-stage-overlap.html">Multi-Stage Overlap</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/muon.html">Muon</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/nsa.html">NSA</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/qk-clip.html">QK-Clip</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/sparse-attention.html">Sparse Attention</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="tag/zero-redundant-communication.html">Zero-Redundant Communication</a>
    </li>
     
  </ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__categories">
  <h3>
    <a href="category.html">Categories</a>
  </h3>
  <ul>
     
    <li>
      <a href="category/magiattention.html">MagiAttention (12)</a>
    </li>
     
  </ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__archives">
  <h3>
    <a href="archive.html">Archives</a>
  </h3>
  <ul>
     
    <li>
      <a href="2026.html">2026 (8)</a>
    </li>
      
    <li>
      <a href="2025.html">2025 (4)</a>
    </li>
     
  </ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__authors">
  <h3>
    <a href="author.html">Authors</a>
  </h3>
  <ul>
     
    <li>
      <a href="author/bowen-zeng.html">Bowen Zeng (3)</a>
    </li>
      
    <li>
      <a href="author/hanwen-sun.html">Hanwen Sun (3)</a>
    </li>
      
    <li>
      <a href="author/jerry-chen.html">Jerry Chen (1)</a>
    </li>
      
    <li>
      <a href="author/jin-li.html">Jin Li (4)</a>
    </li>
      
    <li>
      <a href="author/kunlun-li.html">Kunlun Li (1)</a>
    </li>
      
    <li>
      <a href="author/qiangang-wang.html">Qiangang Wang (4)</a>
    </li>
      
    <li>
      <a href="author/tao-bu.html">Tao Bu (2)</a>
    </li>
      
    <li>
      <a href="author/yufeng-yang.html">Yufeng Yang (1)</a>
    </li>
      
    <li>
      <a href="author/yujia-liu.html">Yujia Liu (1)</a>
    </li>
      
    <li>
      <a href="author/yunpeng-huang.html">Yunpeng Huang (11)</a>
    </li>
      
    <li>
      <a href="author/zewei-tao.html">Zewei Tao (7)</a>
    </li>
     
  </ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__languages">
  <h3>
    <a href="language.html">Languages</a>
  </h3>
  <ul>
     
    <li>
      <a href="language/english.html">English (12)</a>
    </li>
     
  </ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__locations">
  <h3>
    <a href="location.html">Locations</a>
  </h3>
  <ul>
     
    <li>
      <a href="location/china.html">China (12)</a>
    </li>
     
  </ul>
</div>
</div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="toc.html" class="nav-link">Blogs</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Long-Context Attention Benchmark</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                   <section id="long-context-attention-benchmark">
<h1>Long-Context Attention Benchmark<a class="headerlink" href="#long-context-attention-benchmark" title="Link to this heading">#</a></h1>
<p><strong>From Kernel Efficiency to Distributed Scalability</strong></p>
<p>To evaluate the performance and flexibility of <code class="docutils literal notranslate"><span class="pre">Flex-Flash-Attention</span></code> (<code class="docutils literal notranslate"><span class="pre">FFA</span></code>) kernels and to validate the distributed scalability of <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code> for ultra-long, heterogeneous-mask training, we benchmark throughput on modern GPUs (e.g., Hopper and Blackwell) for both kernels and distributed attention modules in forward and backward passes across diverse mask patterns (standard and irregular), against state-of-the-art kernel- and distributed-level baselines.</p>
<section id="benchmark-settings">
<h2>Benchmark Settings<a class="headerlink" href="#benchmark-settings" title="Link to this heading">#</a></h2>
<section id="common-configurations">
<h3>Common Configurations<a class="headerlink" href="#common-configurations" title="Link to this heading">#</a></h3>
<p>To focus on the impact of sequence length and mask pattern, we fix other data and model configurations using common training settings as shown in the table below.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>settings</p></th>
<th class="head"><p>value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>attention type</p></td>
<td><p>self-attention where <code class="docutils literal notranslate"><span class="pre">seqlen</span> <span class="pre">=</span> <span class="pre">seqlen_q</span> <span class="pre">=</span> <span class="pre">seqlen_k</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>batch size (b)</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>number of heads (nh)</p></td>
<td><p>nhq:nhk:nhv = 64:8:8 (GQA)</p></td>
</tr>
<tr class="row-odd"><td><p>head dimension (hd)</p></td>
<td><p>128</p></td>
</tr>
<tr class="row-even"><td><p>dtype</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>window size</p></td>
<td><p>1024 (for sliding window masks only)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="throughput-metrics">
<h3>Throughput Metrics<a class="headerlink" href="#throughput-metrics" title="Link to this heading">#</a></h3>
<p>Throughput is measured in <span class="math notranslate nohighlight">\(\texttt{TFLOPs/s}\)</span> for kernel-level benchmarks and <span class="math notranslate nohighlight">\(\texttt{TFLOPs/s/GPU}\)</span> for distributed benchmarks, calculated based on the total number of floating-point operations (<span class="math notranslate nohighlight">\(\texttt{FLOPs}\)</span>) involved in the attention computation, for both forward and backward passes respectively.</p>
<p>The <span class="math notranslate nohighlight">\(\texttt{FLOPs}\)</span> for each <span class="math notranslate nohighlight">\(\mathrm{AttnSlice}\)</span> are computed using the formula below, and the total <span class="math notranslate nohighlight">\(\texttt{FLOPs}\)</span> is the summation of all <span class="math notranslate nohighlight">\(\mathrm{AttnSlice}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-flops-calculation">
<span class="eqno">(2)<a class="headerlink" href="#equation-flops-calculation" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
  \mathrm{FLOPs}^{(fwd)} &amp;= 2 \times 2 \times\;\; \mathrm{MaskArea}(seqlen, mask\_type) \\
  &amp;\quad \times batch\_size \times num\_heads\_q \times head\_dim \\
  \mathrm{FLOPs}^{(bwd)} &amp;= 2.5 \times\;\; \mathrm{FLOPs}^{(fwd)} \\
  \text{where } \;\;&amp; \mathrm{MaskArea}(seqlen, \text{full}) = seqlen^2, \\
     \;\;&amp; \mathrm{MaskArea}(seqlen, \text{causal}) = \frac{seqlen(seqlen+1)}{2}, \;\; \dots
\end{aligned}\end{split}\]</div>
<p>And the throughputs are calculated as follows:</p>
<div class="math notranslate nohighlight" id="equation-throughput-calculation">
<span class="eqno">(3)<a class="headerlink" href="#equation-throughput-calculation" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
  \mathrm{TFLOPs/s}^{(wd)} &amp;= \cfrac{\mathrm{FLOPs}^{(wd)}}{\mathrm{ElapsedTime}^{(wd)}}, \quad wd \in \{fwd, bwd\} \\
  \mathrm{TFLOPs/s/GPU}^{(wd)} &amp;= \cfrac{\mathrm{FLOPs}^{(wd)}}{\mathrm{ElapsedTime}^{(wd)} \times \mathit{cp\_size}}, \quad wd \in \{fwd, bwd\} \\
  \text{where } \;\;&amp; \mathrm{ElapsedTime}^{(wd)} = \max\limits_{rank \in [0, \mathit{cp\_size})} \mathrm{ElapsedTime}_{rank}^{(wd)}
\end{aligned}\end{split}\]</div>
</section>
<section id="data-distribution-and-sampling">
<h3>Data Distribution and Sampling<a class="headerlink" href="#data-distribution-and-sampling" title="Link to this heading">#</a></h3>
<p>To reflect real-world long-context training, we extract the sequence-length distribution from a representative training dataset and use it to construct variable-length inputs for both kernel- and distributed-level experiments (see <a class="reference internal" href="#varlen-seqlen-distribution"><span class="std std-numref">Fig. 19</span></a>).</p>
<figure class="align-center" id="varlen-seqlen-distribution">
<a class="reference internal image-reference" href="../_images/varlen_seqlen_distribution.png"><img alt="Variable-Length Sequence Distribution" src="../_images/varlen_seqlen_distribution.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">Distribution of sequence lengths extracted from a real-world dataset, which is used to sample and construct the variable-length data for both kernel-level and distributed-level experiments.</span><a class="headerlink" href="#varlen-seqlen-distribution" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>We shuffle the dataset, sequentially pack samples into data packs, then reshuffle those packs to form the final sampling set, where we will fetch a portion of packs for experiments using <code class="docutils literal notranslate"><span class="pre">varlen</span></code> mask patterns. This preserves the original token-length distribution so the probability of tokens from long and short samples within each pack matches the dataset.</p>
<p>To avoid the sampled variable-length data from degenerating into pure <code class="docutils literal notranslate"><span class="pre">full/causal</span></code> masks to affect the evaluation, we limit each sample’s length at most <span class="math notranslate nohighlight">\(\frac{1}{4}\)</span> of the total sequence length (e.g., no sample exceeds <code class="docutils literal notranslate"><span class="pre">16K</span></code> when measuring with a <code class="docutils literal notranslate"><span class="pre">64K</span></code> total sequence length).</p>
</section>
<section id="kernel-baselines">
<h3>Kernel Baselines<a class="headerlink" href="#kernel-baselines" title="Link to this heading">#</a></h3>
<p>On Hopper, we evaluate our <a class="reference internal" href="magi_attn.html#flex-flash-attention"><span class="std std-ref"><code class="docutils literal notranslate"><span class="pre">FFA</span></code></span></a> kernel against widely used PyTorch’s fused <code class="docutils literal notranslate"><span class="pre">SDPA</span></code> <span id="id1">[<a class="reference internal" href="#id33" title="PyTorch. Torch.nn.functional.scaled_dot_product_attention - pytorch 2.6 documentation. https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html.">PyTorch, n.d.</a>]</span>, <code class="docutils literal notranslate"><span class="pre">Flash</span> <span class="pre">Attention</span> <span class="pre">2</span></code> (<code class="docutils literal notranslate"><span class="pre">FA2</span></code>) <span id="id2">[<a class="reference internal" href="#id35" title="Tri Dao. Flashattention-2: faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.">Dao, 2023</a>]</span>, <code class="docutils literal notranslate"><span class="pre">Flash</span> <span class="pre">Attention</span> <span class="pre">3</span></code> (<code class="docutils literal notranslate"><span class="pre">FA3</span></code>) <span id="id3">[<a class="reference internal" href="#id36" title="Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: fast and accurate attention with asynchrony and low-precision. 2024. URL: https://arxiv.org/abs/2407.08608, arXiv:2407.08608.">Shah <em>et al.</em>, 2024</a>]</span>, NVIDIA’s <code class="docutils literal notranslate"><span class="pre">cuDNN</span></code> fused attention kernel <span id="id4">[<a class="reference internal" href="#id40" title="NVIDIA. Accelerating transformers with nvidia cudnn 9. https://developer.nvidia.com/blog/accelerating-transformers-with-nvidia-cudnn-9/, 2024. Accessed: 2024-12-12.">NVIDIA, 2024</a>]</span> from <a class="reference external" href="https://github.com/NVIDIA/TransformerEngine">TransformerEngine</a>, as well as PyTorch’s new <code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code> <span id="id5">[<a class="reference internal" href="#id38" title="Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: a programming model for generating optimized attention kernels. 2024. URL: https://arxiv.org/abs/2412.05496, arXiv:2412.05496.">Dong <em>et al.</em>, 2024</a>]</span> and Baidu’s <code class="docutils literal notranslate"><span class="pre">FlashMask</span></code> <span id="id6">[<a class="reference internal" href="#id39" title="Guoxia Wang, Jinle Zeng, Xiyuan Xiao, Siming Wu, Jiabin Yang, Lujing Zheng, Zeyu Chen, Jiang Bian, Dianhai Yu, and Haifeng Wang. Flashmask: efficient and rich mask extension of flashattention. 2025. URL: https://arxiv.org/abs/2410.01359, arXiv:2410.01359.">Wang <em>et al.</em>, 2025</a>]</span> for baselines on flexible masks.</p>
<p>On Blackwell, we instead evaluate our <a class="reference internal" href="blackwell_ffa_fa4.html"><span class="std std-doc"><code class="docutils literal notranslate"><span class="pre">FFA_FA4</span></code></span></a> kernel against the same baselines, substituting <code class="docutils literal notranslate"><span class="pre">FA2</span></code> and <code class="docutils literal notranslate"><span class="pre">FA3</span></code> with <code class="docutils literal notranslate"><span class="pre">Flash</span> <span class="pre">Attention</span> <span class="pre">4</span></code> (<code class="docutils literal notranslate"><span class="pre">FA4</span></code>) <span id="id7">[<a class="reference internal" href="#id37" title="Tri Dao, Guessous Driss, and Tsang Henry. Flashattention cute module [software documentation]. GitHub Repository README, 2025. URL: https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/cute/README.md.">Dao <em>et al.</em>, 2025</a>]</span>, since both <code class="docutils literal notranslate"><span class="pre">FFA</span></code> and <code class="docutils literal notranslate"><span class="pre">FA3</span></code> are tailored for Hopper and <code class="docutils literal notranslate"><span class="pre">FA2</span></code> does not optimize for SM90+ architectures.</p>
</section>
<section id="distributed-baselines">
<h3>Distributed Baselines<a class="headerlink" href="#distributed-baselines" title="Link to this heading">#</a></h3>
<p>We evaluate <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code> against state-of-the-art distributed attention mechanisms integrated into <a class="reference external" href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a> as context-parallel (CP) backends, including <code class="docutils literal notranslate"><span class="pre">Ulysess</span></code> <span id="id8">[<a class="reference internal" href="#id41" title="Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: system optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. URL: https://arxiv.org/pdf/2309.14509.">Jacobs <em>et al.</em>, 2023</a>]</span>, <code class="docutils literal notranslate"><span class="pre">Ring</span> <span class="pre">P2P</span></code> <span id="id9">[<a class="reference internal" href="#id42" title="Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023.">Liu <em>et al.</em>, 2023</a>]</span>, <code class="docutils literal notranslate"><span class="pre">Ring</span> <span class="pre">AllGather</span></code> <span id="id10">[<a class="reference internal" href="#id49" title="Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models. 2024. URL: https://arxiv.org/abs/2407.21783, arXiv:2407.21783.">Grattafiori <em>et al.</em>, 2024</a>]</span>, <code class="docutils literal notranslate"><span class="pre">USP</span></code> <span id="id11">[<a class="reference internal" href="#id43" title="Jiarui Fang and Shangchun Zhao. Usp: a unified sequence parallelism approach for long context generative ai. 2024. URL: https://arxiv.org/abs/2405.07719, arXiv:2405.07719.">Fang and Zhao, 2024</a>]</span>, <code class="docutils literal notranslate"><span class="pre">LoongTrain</span></code> <span id="id12">[<a class="reference internal" href="#id45" title="Diandian Gu, Peng Sun, Qinghao Hu, Ting Huang, Xun Chen, Yingtong Xiong, Guoteng Wang, Qiaoling Chen, Shangchun Zhao, Jiarui Fang, Yonggang Wen, Tianwei Zhang, Xin Jin, and Xuanzhe Liu. Loongtrain: efficient training of long-sequence llms with head-context parallelism. 2024. URL: https://arxiv.org/abs/2406.18485, arXiv:2406.18485.">Gu <em>et al.</em>, 2024</a>]</span>, and Megatron <code class="docutils literal notranslate"><span class="pre">HybridCP</span></code> <span id="id13">[<a class="reference internal" href="#id48" title="NVIDIA. Megatron-lm pull request #2054. https://github.com/NVIDIA/Megatron-LM/pull/2054, December 2025.">NVIDIA, 2025</a>]</span>. Many of these are discussed in the <a class="reference internal" href="magi_attn.html#related-work"><span class="std std-ref">Related Work</span></a> section of the main MagiAttention <a class="reference internal" href="magi_attn.html"><span class="std std-doc">blog post</span></a>.</p>
<p>On Hopper, all baselines use the <code class="docutils literal notranslate"><span class="pre">FA3</span></code> kernel as the attention backend to ensure a fair comparison with our <code class="docutils literal notranslate"><span class="pre">FFA</span></code> kernel.</p>
<p>On Blackwell, since <code class="docutils literal notranslate"><span class="pre">FA3</span></code> targets Hopper and <code class="docutils literal notranslate"><span class="pre">FA4</span></code> currently lacks robust backward support for varlen masks, baselines use the <code class="docutils literal notranslate"><span class="pre">cuDNN</span></code> kernel while we use our <code class="docutils literal notranslate"><span class="pre">FFA_FA4</span></code> backend. Additionally, Megatron <code class="docutils literal notranslate"><span class="pre">HybridCP</span></code> (which requires <code class="docutils literal notranslate"><span class="pre">FA3</span></code>) is omitted from Blackwell evaluations.</p>
</section>
</section>
<section id="kernel-level">
<h2>Kernel Level<a class="headerlink" href="#kernel-level" title="Link to this heading">#</a></h2>
<p>For kernel-level benchmarking, we evaluate the kernels across 5 common mask patterns including <code class="docutils literal notranslate"><span class="pre">full</span></code>, <code class="docutils literal notranslate"><span class="pre">causal</span></code>, <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">full</span></code>, <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">causal</span></code> and <code class="docutils literal notranslate"><span class="pre">sliding</span> <span class="pre">window</span> <span class="pre">causal</span></code> with one irregular <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">block</span> <span class="pre">causal</span></code> mask used in <a class="reference external" href="https://github.com/SandAI-org/MAGI-1">Magi-1</a>, to assess performance and flexibility, with the total sequence length varying from <code class="docutils literal notranslate"><span class="pre">1K,2K,4K,...,</span></code> up to <code class="docutils literal notranslate"><span class="pre">64K</span></code> for both forward and backward passes.</p>
<p>Results are reported in the following figures, while the legend-name mapping is described below:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>legend</p></th>
<th class="head"><p>name</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ffa</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">FFA</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>fa2 / fa3 / fa4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">FA2</span></code> / <code class="docutils literal notranslate"><span class="pre">FA3</span></code> / <code class="docutils literal notranslate"><span class="pre">FA4</span></code></p></td>
</tr>
<tr class="row-even"><td><p>cudnn</p></td>
<td><p>NVIDIA <code class="docutils literal notranslate"><span class="pre">cuDNN</span></code> fused attention</p></td>
</tr>
<tr class="row-odd"><td><p>sdpa</p></td>
<td><p>PyTorch’s <code class="docutils literal notranslate"><span class="pre">SDPA</span></code></p></td>
</tr>
<tr class="row-even"><td><p>flex</p></td>
<td><p>PyTorch’s <code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>flash_mask</p></td>
<td><p>Baidu’s <code class="docutils literal notranslate"><span class="pre">FlashMask</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> symbol denotes attention kernels unsupported in that configuration due to kernel limitations or error raised (e.g., <code class="docutils literal notranslate"><span class="pre">Cuda</span> <span class="pre">Out</span> <span class="pre">of</span> <span class="pre">Memory</span></code>).</p>
</div>
<section id="for-h100">
<h3>For H100<a class="headerlink" href="#for-h100" title="Link to this heading">#</a></h3>
<section id="full-mask">
<h4>Full Mask<a class="headerlink" href="#full-mask" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-h100-full-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report.png"><img alt="Kernel-Level Throughput - Full Mask Forward Pass" src="../_images/flops_report.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-h100-full-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-h100-full-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report1.png"><img alt="Kernel-Level Throughput - Full Mask Backward Pass" src="../_images/flops_report1.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-h100-full-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA</span></code>’s performance and flexibility against baselines on H100 for the <code class="docutils literal notranslate"><span class="pre">full</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="causal-mask">
<h4>Causal Mask<a class="headerlink" href="#causal-mask" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-h100-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report2.png"><img alt="Kernel-Level Throughput - Causal Mask Forward Pass" src="../_images/flops_report2.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-h100-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-h100-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report3.png"><img alt="Kernel-Level Throughput - Causal Mask Backward Pass" src="../_images/flops_report3.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 23 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-h100-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA</span></code>’s performance and flexibility against baselines on H100 for the <code class="docutils literal notranslate"><span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="varlen-full-mask">
<h4>Varlen Full Mask<a class="headerlink" href="#varlen-full-mask" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-h100-varlen-full-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report4.png"><img alt="Kernel-Level Throughput - Varlen Full Mask Forward Pass" src="../_images/flops_report4.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 24 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-h100-varlen-full-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-h100-varlen-full-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report5.png"><img alt="Kernel-Level Throughput - Varlen Full Mask Backward Pass" src="../_images/flops_report5.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-h100-varlen-full-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA</span></code>’s performance and flexibility against baselines on H100 for the <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">full</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="varlen-causal-mask">
<h4>Varlen Causal Mask<a class="headerlink" href="#varlen-causal-mask" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-h100-varlen-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report6.png"><img alt="Kernel-Level Throughput - Varlen Causal Mask Forward Pass" src="../_images/flops_report6.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 26 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-h100-varlen-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-h100-varlen-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report7.png"><img alt="Kernel-Level Throughput - Varlen Causal Mask Backward Pass" src="../_images/flops_report7.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 27 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-h100-varlen-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA</span></code>’s performance and flexibility against baselines on H100 for the <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="sliding-window-causal-mask">
<h4>Sliding Window Causal Mask<a class="headerlink" href="#sliding-window-causal-mask" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-h100-sw-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report8.png"><img alt="Kernel-Level Throughput - Sliding Window Causal Mask Forward Pass" src="../_images/flops_report8.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 28 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-h100-sw-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-h100-sw-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report9.png"><img alt="Kernel-Level Throughput - Sliding Window Causal Mask Backward Pass" src="../_images/flops_report9.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 29 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-h100-sw-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA</span></code>’s performance and flexibility against baselines on H100 for the <code class="docutils literal notranslate"><span class="pre">sliding</span> <span class="pre">window</span> <span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="varlen-block-causal-mask">
<h4>Varlen Block Causal Mask 🔥<a class="headerlink" href="#varlen-block-causal-mask" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-h100-varlen-block-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report10.png"><img alt="Kernel-Level Throughput - Varlen Block Causal Mask Forward Pass" src="../_images/flops_report10.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 30 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-h100-varlen-block-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-h100-varlen-block-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report11.png"><img alt="Kernel-Level Throughput - Varlen Block Causal Mask Backward Pass" src="../_images/flops_report11.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 31 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-h100-varlen-block-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA</span></code>’s performance and flexibility against baselines on H100 for the <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">block</span> <span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
</section>
<section id="for-b200">
<h3>For B200<a class="headerlink" href="#for-b200" title="Link to this heading">#</a></h3>
<section id="id14">
<h4>Full Mask<a class="headerlink" href="#id14" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-b200-full-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report12.png"><img alt="Kernel-Level Throughput - Full Mask Forward Pass" src="../_images/flops_report12.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 32 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-b200-full-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-b200-full-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report13.png"><img alt="Kernel-Level Throughput - Full Mask Backward Pass" src="../_images/flops_report13.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 33 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-b200-full-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA_FA4</span></code>’s performance and flexibility against baselines on B200 for the <code class="docutils literal notranslate"><span class="pre">full</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="id15">
<h4>Causal Mask<a class="headerlink" href="#id15" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-b200-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report14.png"><img alt="Kernel-Level Throughput - Causal Mask Forward Pass" src="../_images/flops_report14.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 34 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-b200-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-b200-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report15.png"><img alt="Kernel-Level Throughput - Causal Mask Backward Pass" src="../_images/flops_report15.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 35 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-b200-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA_FA4</span></code>’s performance and flexibility against baselines on B200 for the <code class="docutils literal notranslate"><span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="id16">
<h4>Varlen Full Mask<a class="headerlink" href="#id16" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-b200-varlen-full-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report16.png"><img alt="Kernel-Level Throughput - Varlen Full Mask Forward Pass" src="../_images/flops_report16.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 36 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-b200-varlen-full-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-b200-varlen-full-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report17.png"><img alt="Kernel-Level Throughput - Varlen Full Mask Backward Pass" src="../_images/flops_report17.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 37 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-b200-varlen-full-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA_FA4</span></code>’s performance and flexibility against baselines on B200 for the <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">full</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="id17">
<h4>Varlen Causal Mask<a class="headerlink" href="#id17" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-b200-varlen-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report18.png"><img alt="Kernel-Level Throughput - Varlen Causal Mask Forward Pass" src="../_images/flops_report18.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 38 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-b200-varlen-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-b200-varlen-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report19.png"><img alt="Kernel-Level Throughput - Varlen Causal Mask Backward Pass" src="../_images/flops_report19.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 39 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-b200-varlen-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA_FA4</span></code>’s performance and flexibility against baselines on B200 for the <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="id18">
<h4>Sliding Window Causal Mask<a class="headerlink" href="#id18" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-b200-sw-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report20.png"><img alt="Kernel-Level Throughput - Sliding Window Causal Mask Forward Pass" src="../_images/flops_report20.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 40 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-b200-sw-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-b200-sw-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report21.png"><img alt="Kernel-Level Throughput - Sliding Window Causal Mask Backward Pass" src="../_images/flops_report21.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 41 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-b200-sw-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA_FA4</span></code>’s performance and flexibility against baselines on B200 for the <code class="docutils literal notranslate"><span class="pre">sliding</span> <span class="pre">window</span> <span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="id19">
<h4>Varlen Block Causal Mask 🔥<a class="headerlink" href="#id19" title="Link to this heading">#</a></h4>
<figure class="align-center" id="kernel-tflops-b200-varlen-block-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report22.png"><img alt="Kernel-Level Throughput - Varlen Block Causal Mask Forward Pass" src="../_images/flops_report22.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 42 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#kernel-tflops-b200-varlen-block-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="kernel-tflops-b200-varlen-block-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report23.png"><img alt="Kernel-Level Throughput - Varlen Block Causal Mask Backward Pass" src="../_images/flops_report23.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 43 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#kernel-tflops-b200-varlen-block-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">FFA_FA4</span></code>’s performance and flexibility against baselines on B200 for the <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">block</span> <span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
</section>
</section>
<section id="distributed-level">
<h2>Distributed Level<a class="headerlink" href="#distributed-level" title="Link to this heading">#</a></h2>
<p>For distributed-level benchmarking, we evaluate the CP strategies across 4 common mask patterns including <code class="docutils literal notranslate"><span class="pre">full</span></code>, <code class="docutils literal notranslate"><span class="pre">causal</span></code>, <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">full</span></code> and <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">causal</span></code>, to assess performance and scalability, with the <code class="docutils literal notranslate"><span class="pre">cp_size</span></code> scaling from <code class="docutils literal notranslate"><span class="pre">8</span></code> up to <code class="docutils literal notranslate"><span class="pre">64</span></code> for both forward and backward passes.</p>
<p>As for the total sequence length, we scale it linearly together with <code class="docutils literal notranslate"><span class="pre">cp_size</span></code> and fix the per-device sequence length to reflect the common training configuration w.r.t. the GPU memory capacity, e.g. <code class="docutils literal notranslate"><span class="pre">8K</span></code> for H100 and <code class="docutils literal notranslate"><span class="pre">16K</span></code> for B200.</p>
<p>Results are reported in the following figures, while the legend-name mapping is described below:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>legend</p></th>
<th class="head"><p>name</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>magi_attn-a2av</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code> with <a class="reference internal" href="magi_attn.html#alltoall-v-implementation"><span class="std std-ref"><code class="docutils literal notranslate"><span class="pre">AlltoAll-v</span></code>-based group collectives</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>magi_attn-native</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code> with <a class="reference internal" href="magi_attn.html#native-implementation"><span class="std std-ref">native group collectives</span></a></p></td>
</tr>
<tr class="row-even"><td><p>ulysses</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ulysses</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>ring_p2p</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ring</span> <span class="pre">P2P</span></code></p></td>
</tr>
<tr class="row-even"><td><p>ring_allgather</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ring</span> <span class="pre">AllGather</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>usp</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">USP</span></code></p></td>
</tr>
<tr class="row-even"><td><p>loongtrain</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LoongTrain</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>hybrid_dcp</p></td>
<td><p>Megatron <code class="docutils literal notranslate"><span class="pre">HybridCP</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code>, we include two instances with different backends of group collectives: one using the original <code class="docutils literal notranslate"><span class="pre">AlltoAll-v</span></code>-based implementation and the other using native kernel based on DeepEP <span id="id20">[<a class="reference internal" href="#id50" title="Chenggang Zhao, Shangyan Zhou, Liyue Zhang, Chengqi Deng, Zhean Xu, Yuxuan Liu, Kuai Yu, Jiashi Li, and Liang Zhao. Deepep: an efficient expert-parallel communication library. https://github.com/deepseek-ai/DeepEP, 2025.">Zhao <em>et al.</em>, 2025</a>]</span>, to demonstrate the significant gain from our new native backend.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>We’ve applied some experimental features on <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code> to further optimize the performance on benchmarking, which may not be enabled by default or fully ready for production use yet.</p>
<p>Therefore, the benchmarking results of <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code> in this section are intended to demonstrate the potential performance and scalability of our design, while the actual performance in production may vary and require to be tuned specifically.</p>
<p>We will continue to optimize and stabilize those features and ease the adoption in production, and very welcome users to try out those features and provide feedback to us.</p>
</div>
<section id="id21">
<h3>For H100<a class="headerlink" href="#id21" title="Link to this heading">#</a></h3>
<section id="id22">
<h4>Full Mask<a class="headerlink" href="#id22" title="Link to this heading">#</a></h4>
<figure class="align-center" id="distributed-tflops-per-gpu-h100-full-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report24.png"><img alt="Distributed-Level Throughput - Full Mask Forward Pass" src="../_images/flops_report24.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 44 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-h100-full-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="distributed-tflops-per-gpu-h100-full-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report25.png"><img alt="Distributed-Level Throughput - Full Mask Backward Pass" src="../_images/flops_report25.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 45 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-h100-full-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code>’s performance and scalability against baselines on H100 for the <code class="docutils literal notranslate"><span class="pre">full</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="id23">
<h4>Causal Mask<a class="headerlink" href="#id23" title="Link to this heading">#</a></h4>
<figure class="align-center" id="distributed-tflops-per-gpu-h100-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report26.png"><img alt="Distributed-Level Throughput - Causal Mask Forward Pass" src="../_images/flops_report26.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 46 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-h100-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="distributed-tflops-per-gpu-h100-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report27.png"><img alt="Distributed-Level Throughput - Causal Mask Backward Pass" src="../_images/flops_report27.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 47 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-h100-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code>’s performance and scalability against baselines on H100 for the <code class="docutils literal notranslate"><span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="id24">
<h4>Varlen Full Mask 🔥<a class="headerlink" href="#id24" title="Link to this heading">#</a></h4>
<figure class="align-center" id="distributed-tflops-per-gpu-h100-varlen-full-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report28.png"><img alt="Distributed-Level Throughput - Varlen Full Mask Forward Pass" src="../_images/flops_report28.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 48 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-h100-varlen-full-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="distributed-tflops-per-gpu-h100-varlen-full-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report29.png"><img alt="Distributed-Level Throughput - Varlen Full Mask Backward Pass" src="../_images/flops_report29.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 49 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-h100-varlen-full-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code>’s performance and scalability against baselines on H100 for the <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">full</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="id25">
<h4>Varlen Causal Mask 🔥<a class="headerlink" href="#id25" title="Link to this heading">#</a></h4>
<figure class="align-center" id="distributed-tflops-per-gpu-h100-varlen-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report30.png"><img alt="Distributed-Level Throughput - Varlen Causal Mask Forward Pass" src="../_images/flops_report30.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 50 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-h100-varlen-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="distributed-tflops-per-gpu-h100-varlen-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report31.png"><img alt="Distributed-Level Throughput - Varlen Causal Mask Backward Pass" src="../_images/flops_report31.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 51 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-h100-varlen-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code>’s performance and scalability against baselines on H100 for the <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
</section>
<section id="id26">
<h3>For B200<a class="headerlink" href="#id26" title="Link to this heading">#</a></h3>
<section id="id27">
<h4>Full Mask<a class="headerlink" href="#id27" title="Link to this heading">#</a></h4>
<figure class="align-center" id="distributed-tflops-per-gpu-b200-full-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report32.png"><img alt="Distributed-Level Throughput - Full Mask Forward Pass" src="../_images/flops_report32.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 52 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-b200-full-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="distributed-tflops-per-gpu-b200-full-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report33.png"><img alt="Distributed-Level Throughput - Full Mask Backward Pass" src="../_images/flops_report33.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 53 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-b200-full-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code>’s performance and scalability against baselines on B200 for the <code class="docutils literal notranslate"><span class="pre">full</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="id28">
<h4>Causal Mask<a class="headerlink" href="#id28" title="Link to this heading">#</a></h4>
<figure class="align-center" id="distributed-tflops-per-gpu-b200-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report34.png"><img alt="Distributed-Level Throughput - Causal Mask Forward Pass" src="../_images/flops_report34.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 54 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-b200-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="distributed-tflops-per-gpu-b200-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report35.png"><img alt="Distributed-Level Throughput - Causal Mask Backward Pass" src="../_images/flops_report35.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 55 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-b200-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code>’s performance and scalability against baselines on B200 for the <code class="docutils literal notranslate"><span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="id29">
<h4>Varlen Full Mask 🔥<a class="headerlink" href="#id29" title="Link to this heading">#</a></h4>
<figure class="align-center" id="distributed-tflops-per-gpu-b200-varlen-full-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report36.png"><img alt="Distributed-Level Throughput - Varlen Full Mask Forward Pass" src="../_images/flops_report36.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 56 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-b200-varlen-full-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="distributed-tflops-per-gpu-b200-varlen-full-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report37.png"><img alt="Distributed-Level Throughput - Varlen Full Mask Backward Pass" src="../_images/flops_report37.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 57 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-b200-varlen-full-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code>’s performance and scalability against baselines on B200 for the <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">full</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
<section id="id30">
<h4>Varlen Causal Mask 🔥<a class="headerlink" href="#id30" title="Link to this heading">#</a></h4>
<figure class="align-center" id="distributed-tflops-per-gpu-b200-varlen-causal-mask-fwd">
<a class="reference internal image-reference" href="../_images/flops_report38.png"><img alt="Distributed-Level Throughput - Varlen Causal Mask Forward Pass" src="../_images/flops_report38.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 58 </span><span class="caption-text">(a) Forward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-b200-varlen-causal-mask-fwd" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="distributed-tflops-per-gpu-b200-varlen-causal-mask-bwd">
<a class="reference internal image-reference" href="../_images/flops_report39.png"><img alt="Distributed-Level Throughput - Varlen Causal Mask Backward Pass" src="../_images/flops_report39.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 59 </span><span class="caption-text">(b) Backward Pass</span><a class="headerlink" href="#distributed-tflops-per-gpu-b200-varlen-causal-mask-bwd" title="Link to this image">#</a></p>
<div class="legend">
<p>Benchmarking <code class="docutils literal notranslate"><span class="pre">MagiAttention</span></code>’s performance and scalability against baselines on B200 for the <code class="docutils literal notranslate"><span class="pre">varlen</span> <span class="pre">causal</span></code> mask.</p>
</div>
</figcaption>
</figure>
</section>
</section>
</section>
<section id="citation">
<h2>Citation<a class="headerlink" href="#citation" title="Link to this heading">#</a></h2>
<p>If you find MagiAttention useful in your research, please cite:</p>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@misc</span><span class="p">{</span><span class="nl">magiattention2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{MagiAttention: A Distributed Attention Towards Linear Scalability for Ultra-Long Context, Heterogeneous Mask Training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zewei, Tao and Yunpeng, Huang}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">howpublished</span><span class="p">=</span><span class="s">{\url{https://github.com/SandAI-org/MagiAttention/}}</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id31">
<div role="list" class="citation-list">
<div class="citation" id="id35" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">1</a><span class="fn-bracket">]</span></span>
<p>Tri Dao. Flashattention-2: faster attention with better parallelism and work partitioning. <em>arXiv preprint arXiv:2307.08691</em>, 2023.</p>
</div>
<div class="citation" id="id37" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">2</a><span class="fn-bracket">]</span></span>
<p>Tri Dao, Guessous Driss, and Tsang Henry. Flashattention cute module [software documentation]. GitHub Repository README, 2025. URL: <a class="github reference external" href="https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/cute/README.md">Dao-AILab/flash-attention</a>.</p>
</div>
<div class="citation" id="id38" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">3</a><span class="fn-bracket">]</span></span>
<p>Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: a programming model for generating optimized attention kernels. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2412.05496">https://arxiv.org/abs/2412.05496</a>, <a class="reference external" href="https://arxiv.org/abs/2412.05496">arXiv:2412.05496</a>.</p>
</div>
<div class="citation" id="id43" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">4</a><span class="fn-bracket">]</span></span>
<p>Jiarui Fang and Shangchun Zhao. Usp: a unified sequence parallelism approach for long context generative ai. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2405.07719">https://arxiv.org/abs/2405.07719</a>, <a class="reference external" href="https://arxiv.org/abs/2405.07719">arXiv:2405.07719</a>.</p>
</div>
<div class="citation" id="id49" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">5</a><span class="fn-bracket">]</span></span>
<p>Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2407.21783">https://arxiv.org/abs/2407.21783</a>, <a class="reference external" href="https://arxiv.org/abs/2407.21783">arXiv:2407.21783</a>.</p>
</div>
<div class="citation" id="id45" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">6</a><span class="fn-bracket">]</span></span>
<p>Diandian Gu, Peng Sun, Qinghao Hu, Ting Huang, Xun Chen, Yingtong Xiong, Guoteng Wang, Qiaoling Chen, Shangchun Zhao, Jiarui Fang, Yonggang Wen, Tianwei Zhang, Xin Jin, and Xuanzhe Liu. Loongtrain: efficient training of long-sequence llms with head-context parallelism. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2406.18485">https://arxiv.org/abs/2406.18485</a>, <a class="reference external" href="https://arxiv.org/abs/2406.18485">arXiv:2406.18485</a>.</p>
</div>
<div class="citation" id="id41" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">7</a><span class="fn-bracket">]</span></span>
<p>Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: system optimizations for enabling training of extreme long sequence transformer models. <em>arXiv preprint arXiv:2309.14509</em>, 2023. URL: <a class="reference external" href="https://arxiv.org/pdf/2309.14509">https://arxiv.org/pdf/2309.14509</a>.</p>
</div>
<div class="citation" id="id42" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">8</a><span class="fn-bracket">]</span></span>
<p>Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. <em>arXiv preprint arXiv:2310.01889</em>, 2023.</p>
</div>
<div class="citation" id="id40" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">9</a><span class="fn-bracket">]</span></span>
<p>NVIDIA. Accelerating transformers with nvidia cudnn 9. <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-transformers-with-nvidia-cudnn-9/">https://developer.nvidia.com/blog/accelerating-transformers-with-nvidia-cudnn-9/</a>, 2024. Accessed: 2024-12-12.</p>
</div>
<div class="citation" id="id33" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">10</a><span class="fn-bracket">]</span></span>
<p>PyTorch. Torch.nn.functional.scaled_dot_product_attention - pytorch 2.6 documentation. <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html</a>.</p>
</div>
<div class="citation" id="id36" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">11</a><span class="fn-bracket">]</span></span>
<p>Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: fast and accurate attention with asynchrony and low-precision. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2407.08608">https://arxiv.org/abs/2407.08608</a>, <a class="reference external" href="https://arxiv.org/abs/2407.08608">arXiv:2407.08608</a>.</p>
</div>
<div class="citation" id="id39" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">12</a><span class="fn-bracket">]</span></span>
<p>Guoxia Wang, Jinle Zeng, Xiyuan Xiao, Siming Wu, Jiabin Yang, Lujing Zheng, Zeyu Chen, Jiang Bian, Dianhai Yu, and Haifeng Wang. Flashmask: efficient and rich mask extension of flashattention. 2025. URL: <a class="reference external" href="https://arxiv.org/abs/2410.01359">https://arxiv.org/abs/2410.01359</a>, <a class="reference external" href="https://arxiv.org/abs/2410.01359">arXiv:2410.01359</a>.</p>
</div>
<div class="citation" id="id50" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">13</a><span class="fn-bracket">]</span></span>
<p>Chenggang Zhao, Shangyan Zhou, Liyue Zhang, Chengqi Deng, Zhean Xu, Yuxuan Liu, Kuai Yu, Jiashi Li, and Liang Zhao. Deepep: an efficient expert-parallel communication library. <a class="github reference external" href="https://github.com/deepseek-ai/DeepEP">deepseek-ai/DeepEP</a>, 2025.</p>
</div>
<div class="citation" id="id48" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">14</a><span class="fn-bracket">]</span></span>
<p>NVIDIA. Megatron-lm pull request #2054. <a class="github reference external" href="https://github.com/NVIDIA/Megatron-LM/pull/2054">NVIDIA/Megatron-LM#2054</a>, December 2025.</p>
</div>
</div>
</div>
</section>
</section>

<div class="section ablog__blog_comments">
     
<div class="section ablog__prev-next">
  <span class="ablog__prev">
     
    <a href="magi_attn.html">
      
      <i class="fa fa-arrow-circle-left"></i>
      
      <span>MagiAttention</span>
    </a>
    
  </span>
  <span class="ablog__spacer">&nbsp;</span>
  <span class="ablog__next">
     
    <a href="attn_sink.html">
      <span>Support Learnable Attention Sink</span>
      
      <i class="fa fa-arrow-circle-right"></i>
      
    </a>
    
  </span>
</div>
  
</div>

                </article>
              
              
              
              
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-settings">Benchmark Settings</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-configurations">Common Configurations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#throughput-metrics">Throughput Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-distribution-and-sampling">Data Distribution and Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-baselines">Kernel Baselines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-baselines">Distributed Baselines</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-level">Kernel Level</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#for-h100">For H100</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#full-mask">Full Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#causal-mask">Causal Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#varlen-full-mask">Varlen Full Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#varlen-causal-mask">Varlen Causal Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sliding-window-causal-mask">Sliding Window Causal Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#varlen-block-causal-mask">Varlen Block Causal Mask 🔥</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#for-b200">For B200</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Full Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Causal Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Varlen Full Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Varlen Causal Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Sliding Window Causal Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Varlen Block Causal Mask 🔥</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-level">Distributed Level</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">For H100</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Full Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Causal Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">Varlen Full Mask 🔥</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">Varlen Causal Mask 🔥</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">For B200</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">Full Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">Causal Mask</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">Varlen Full Mask 🔥</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">Varlen Causal Mask 🔥</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#citation">Citation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025-2026, Sandai.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 9.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>